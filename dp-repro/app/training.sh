#!/usr/bin/env bash
set -euo pipefail

MASTER_NODE_ADDRESS="train-workers-0-0.train"
MASTER_NODE_PORT=3389
NODE_RANK="$NODE_RANK"
NUM_NODES="$NODE_COUNT"
NUM_GPU_PER_NODE=8
BATCH_SIZE_PER_GPU="$6"
ACCUMULATION_BATCH_SIZE="$7"

if [ -z "$MASTER_NODE_ADDRESS" ] || [ -z "$MASTER_NODE_PORT" ] || [ -z "$NODE_RANK" ] || [ -z "$NUM_NODES" ] || [ -z "$NUM_GPU_PER_NODE" ] || [ -z "$BATCH_SIZE_PER_GPU" ] || [ -z "$ACCUMULATION_BATCH_SIZE" ]; then
    echo "Usage: $0 <MASTER_NODE_ADDRESS> <MASTER_NODE_PORT> <NODE_RANK> <NUM_NODES> <NUM_GPU_PER_NODE> <BATCH_SIZE_PER_GPU> <ACCUMULATION_BATCH_SIZE>"
    exit 1
fi

echo "Running on node rank $NODE_RANK of $NUM_NODES nodes with $NUM_GPU_PER_NODE GPUs."

# Set up NCCL environment variables
source /usr/local/gib/scripts/set_nccl_env.sh

echo "$(cat /usr/local/gib/scripts/set_nccl_env.sh)"

echo ""
echo "Environment variables set:"
echo "$(env)"
echo ""

# Write deepspeed env vars
cat <<EOT >~/.deepspeed_env
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export NCCL_DEBUG="INFO"
EOT

for line in $(env | grep NCCL)
do
    echo "export $line" >> ~/.deepspeed_env
done

echo "DeepSpeed environment variables:"
echo "$(cat ~/.deepspeed_env)"
echo ""

# Write deepspeed config
cat <<EOT >ds_config.json
{
 "fp16": {
 "enabled": false
 },
 "bf16": {
 "enabled": true
 },
 "optimizer": {
 "type": "Adam",
 "params": {
 "lr": "auto",
 "betas": [0.9, 0.999],
 "eps": 1e-8,
 "weight_decay": "auto",
 "torch_adam": true,
 "adam_w_mode": true
 }
 },
 "scheduler": {
 "type": "WarmupCosineLR",
 "params": {
 "total_num_steps": "auto",
 "warmup_min_ratio": 0.03,
 "warmup_num_steps": "auto"
 }
 },
  
"zero_optimization": {
 "stage": 3,
 "offload_optimizer": {
 "device": "none"
},
"offload_param": {
"device": "none"
},
 "overlap_comm": true,
 "contiguous_gradients": true,
 "sub_group_size": 1e9,
 "reduce_bucket_size": 3e9,
 "stage3_prefetch_bucket_size": 3e9,
 "stage3_param_persistence_threshold": 1e6,
 "stage3_max_live_parameters": 1.5e9,
 "stage3_max_reuse_distance": 1e9,
 "stage3_gather_16bit_weights_on_model_save": true,
 "memory_efficient_linear": false,
 "round_robin_gradients": true
 },
 "gradient_accumulation_steps": "auto",
 "gradient_clipping": "auto",
 "steps_per_print": 10,
 "train_batch_size": "auto",
 "train_micro_batch_size_per_gpu": "auto",
 "wall_clock_breakdown": false,
 "activation_checkpointing": {
 "partition_activations": true,
 "contiguous_memory_optimization": true,
 "number_checkpoints": 4
 }
}
EOT


cat <<EOT >hostfile
# This file is automatically generated by the training script.
# It contains the list of nodes and their ranks.
# Format: <node_name> slots=<num_slots>
$MASTER_NODE_ADDRESS slots=$NUM_GPU_PER_NODE
EOT

HOSTFILE="$(realpath hostfile)"
for ((i=1; i<NUM_NODES; i++)); do
    node_addr="$(echo $MASTER_NODE_ADDRESS | sed 's/0-0/0-'"${i}"'/g')"
    echo "$node_addr slots=$NUM_GPU_PER_NODE" >> "$HOSTFILE"
done

echo "Deepspeed hostfile with the following content:"
echo "$(cat $HOSTFILE)"
echo ""

NUM_CPU_CORES=${8:-8}
DATA_DIR="/mount/data"
DEEPSPEED_CONFIG="$(realpath ds_config.json)"
MODEL="/mount/base-model"
MODEL_OUTPUT_NAME="trained-model"
#TRAIN_FILE="$DATA_DIR/10K.csv"
#EVAL_FILE="$DATA_DIR/1K.csv"
TRAIN_FILE="$DATA_DIR/train-00000-of-00001.parquet"
EVAL_FILE="$DATA_DIR/validation-00000-of-00001.parquet"
OUTPUT_DIR="/mount/local-checkpoints"

mkdir -p "$OUTPUT_DIR"

echo "Listing directory contents."
echo "ls -l $DATA_DIR"
echo "$(ls -l $DATA_DIR)"
echo ""
echo "ls -l $MODEL"
echo "$(ls -l $MODEL)"
echo ""

# Ensure that ACCUMULATION_BATCH_SIZE is fully divisible by NUM_NODES * NUM_GPU_PER_NODE * BATCH_SIZE_PER_GPU
ACCUMULATION_STEPS="$((ACCUMULATION_BATCH_SIZE/NUM_NODES/NUM_GPU_PER_NODE/BATCH_SIZE_PER_GPU))"

echo ""
echo "Starting training using:"
echo "- master node address $MASTER_NODE_ADDRESS"
echo "- port $MASTER_NODE_PORT"
echo "- node rank $NODE_RANK"
echo "- total nodes $NUM_NODES"
echo "- GPUs per node $NUM_GPU_PER_NODE"
echo "- batch size per GPU $BATCH_SIZE_PER_GPU"
echo "- and accumulation batch size $ACCUMULATION_BATCH_SIZE."
echo ""

poetry run python3 -c "import torch; print('Torch version:', torch.__version__); print('CUDA available:', torch.cuda.is_available()); print('NCCL version:', torch.cuda.nccl.version())"


LOGFILE="$OUTPUT_DIR/log-$NODE_RANK.txt"
>$LOGFILE
PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True" OMP_NUM_THREADS=$NUM_CPU_CORES poetry run deepspeed \
    --hostfile=$HOSTFILE --no_ssh \
    --node_rank=$NODE_RANK \
    --master_addr=$MASTER_NODE_ADDRESS \
    --master_port=$MASTER_NODE_PORT \
    run_clm_sft_mlflow.py \
    --report_to mlflow \
    --attn_implementation flash_attention_2 \
    --bf16 \
    --torch_dtype bfloat16 \
    --use_auth_token True \
    --logging_strategy steps \
    --logging_steps 24 \
    --save_strategy steps \
    --save_steps 800 \
    --save_total_limit 10 \
    --num_train_epochs 1 \
    --learning_rate 4e-4 \
    --warmup_steps 400 \
    --model_name_or_path $MODEL \
    --max_source_length 2048\
    --max_target_length 1024 \
    --do_train \
    --train_file $TRAIN_FILE \
    --validation_file $EVAL_FILE \
    --text_column hinted_prompt \
    --summary_column response \
    --output_dir $OUTPUT_DIR \
    --overwrite_output_dir \
    --per_device_train_batch_size $BATCH_SIZE_PER_GPU \
    --gradient_accumulation_steps $ACCUMULATION_STEPS \
    --eval_strategy no \
    --per_device_eval_batch_size 1 \
    --ddp_backend nccl \
    --ddp_timeout 36000 \
    --preprocessing_num_workers $NUM_CPU_CORES \
    --weight_decay 0.0001 \
    --warmup_ratio 0 \
    --max_grad_norm 0.3 \
    --max_steps 1024 \
    --logging_steps 24 --eval_steps 48 \
    --deepspeed $DEEPSPEED_CONFIG

echo "Training completed."
