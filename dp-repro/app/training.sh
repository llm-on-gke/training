#!/usr/bin/env bash
set -euo pipefail

MASTER_NODE_ADDRESS="train-workers-0-0.train"
MASTER_NODE_PORT=3389
NODE_RANK="$NODE_RANK"
NUM_NODES="$NODE_COUNT"
NUM_GPU_PER_NODE=8
BATCH_SIZE_PER_GPU="$6"
ACCUMULATION_BATCH_SIZE="$7"

if [ -z "$MASTER_NODE_ADDRESS" ] || [ -z "$MASTER_NODE_PORT" ] || [ -z "$NODE_RANK" ] || [ -z "$NUM_NODES" ] || [ -z "$NUM_GPU_PER_NODE" ] || [ -z "$BATCH_SIZE_PER_GPU" ] || [ -z "$ACCUMULATION_BATCH_SIZE" ]; then
    echo "Usage: $0 <MASTER_NODE_ADDRESS> <MASTER_NODE_PORT> <NODE_RANK> <NUM_NODES> <NUM_GPU_PER_NODE> <BATCH_SIZE_PER_GPU> <ACCUMULATION_BATCH_SIZE>"
    exit 1
fi

echo "Running on node rank $NODE_RANK of $NUM_NODES nodes with $NUM_GPU_PER_NODE GPUs."

# Set up NCCL environment variables
source /usr/local/gib/scripts/set_nccl_env.sh

echo "$(cat /usr/local/gib/scripts/set_nccl_env.sh)"

echo ""
echo "Environment variables set:"
echo "$(env)"
echo ""

# Write deepspeed env vars
cat <<EOT >~/.deepspeed_env
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export NCCL_DEBUG="INFO"
EOT

for line in $(env | grep NCCL)
do
    echo "export $line" >> ~/.deepspeed_env
done

echo "DeepSpeed environment variables:"
echo "$(cat ~/.deepspeed_env)"
echo ""

# Write deepspeed config
cat <<EOT >ds_config.json
{
    "bfloat16": {
        "enabled": true
    },
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": "auto",
            "betas": "auto",
            "eps": "auto",
            "weight_decay": "auto"
        }
    },
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "last_batch_iteration": -1,
            "total_num_steps": "auto",
            "warmup_min_lr": "auto",
            "warmup_max_lr": "auto",
            "warmup_num_steps": "auto"
        }
    },
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "none",
            "pin_memory": true
        },
        "offload_param": {
            "device": "none",
            "pin_memory": true
        },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_scatter": true,
        "stage3_prefetch_bucket_size": 1e9,
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": true,
        "zero_hpz_partition_size": $NUM_GPU_PER_NODE,
        "zero_quantized_gradients": false,
        "zero_quantized_weights": false, 
	    "reduce_bucket_size": "auto" 
    },
    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "steps_per_print": 1,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "communication_data_type": "fp32",
    "wall_clock_breakdown": false,
    "checkpoint": {
        "use_node_local_storage": true,
        "load_universal": true
    }
}
EOT


cat <<EOT >hostfile
# This file is automatically generated by the training script.
# It contains the list of nodes and their ranks.
# Format: <node_name> slots=<num_slots>
$MASTER_NODE_ADDRESS slots=$NUM_GPU_PER_NODE
EOT

HOSTFILE="$(realpath hostfile)"
for ((i=1; i<NUM_NODES; i++)); do
    node_addr="$(echo $MASTER_NODE_ADDRESS | sed 's/0-0/0-'"${i}"'/g')"
    echo "$node_addr slots=$NUM_GPU_PER_NODE" >> "$HOSTFILE"
done

echo "Deepspeed hostfile with the following content:"
echo "$(cat $HOSTFILE)"
echo ""

NUM_CPU_CORES=${8:-8}
DATA_DIR="/mount/data"
DEEPSPEED_CONFIG="$(realpath ds_config.json)"
MODEL="/mount/base-model"
MODEL_OUTPUT_NAME="trained-model"
TRAIN_FILE="$DATA_DIR/10K.csv"
EVAL_FILE="$DATA_DIR/1K.csv"
OUTPUT_DIR="/mount/local-checkpoints"

mkdir -p "$OUTPUT_DIR"

echo "Listing directory contents."
echo "ls -l $DATA_DIR"
echo "$(ls -l $DATA_DIR)"
echo ""
echo "ls -l $MODEL"
echo "$(ls -l $MODEL)"
echo ""

# Ensure that ACCUMULATION_BATCH_SIZE is fully divisible by NUM_NODES * NUM_GPU_PER_NODE * BATCH_SIZE_PER_GPU
ACCUMULATION_STEPS="$((ACCUMULATION_BATCH_SIZE/NUM_NODES/NUM_GPU_PER_NODE/BATCH_SIZE_PER_GPU))"

echo ""
echo "Starting training using:"
echo "- master node address $MASTER_NODE_ADDRESS"
echo "- port $MASTER_NODE_PORT"
echo "- node rank $NODE_RANK"
echo "- total nodes $NUM_NODES"
echo "- GPUs per node $NUM_GPU_PER_NODE"
echo "- batch size per GPU $BATCH_SIZE_PER_GPU"
echo "- and accumulation batch size $ACCUMULATION_BATCH_SIZE."
echo ""

poetry run python3 -c "import torch; print('Torch version:', torch.__version__); print('CUDA available:', torch.cuda.is_available()); print('NCCL version:', torch.cuda.nccl.version())"


LOGFILE="$OUTPUT_DIR/log-$NODE_RANK.txt"
>$LOGFILE

OMP_NUM_THREADS=$NUM_CPU_CORES poetry run deepspeed \
    --hostfile=$HOSTFILE --no_ssh \
    --node_rank=$NODE_RANK \
    --master_addr=$MASTER_NODE_ADDRESS \
    --master_port=$MASTER_NODE_PORT \
    run_clm_sft_mlflow.py \
    --report_to mlflow \
    --attn_implementation flash_attention_2 \
    --bf16 \
    --torch_dtype bfloat16 \
    --use_auth_token True \
    --logging_strategy steps \
    --logging_steps 1 \
    --save_strategy steps \
    --save_steps 800 \
    --save_total_limit 10 \
    --num_train_epochs 1 \
    --learning_rate 2e-6 \
    --warmup_steps 400 \
    --model_name_or_path $MODEL \
    --max_source_length 4000 \
    --max_target_length 2000 \
    --do_train \
    --train_file $TRAIN_FILE \
    --validation_file $EVAL_FILE \
    --text_column hinted_prompt \
    --summary_column response \
    --output_dir $OUTPUT_DIR \
    --overwrite_output_dir \
    --per_device_train_batch_size $BATCH_SIZE_PER_GPU \
    --gradient_accumulation_steps $ACCUMULATION_STEPS \
    --eval_strategy no \
    --per_device_eval_batch_size 1 \
    --ddp_backend nccl \
    --ddp_timeout 36000 \
    --gradient_checkpointing \
    --preprocessing_num_workers $NUM_CPU_CORES \
    --deepspeed $DEEPSPEED_CONFIG

echo "Training completed."
