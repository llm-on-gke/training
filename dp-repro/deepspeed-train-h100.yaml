apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  #namespace: default
  name: train
  #labels:
  #     kueue.x-k8s.io/queue-name: dws-local-queue
  #annotations:
  #    provreq.kueue.x-k8s.io/maxRunDurationHours: "10"
  #annotations:
  #  kueue.x-k8s.io/queue-name: ${K8S_LOCAL_QUEUE}
spec:
  network:
    enableDNSHostnames: true
  replicatedJobs:
  - name: workers
    template:
      spec:
        parallelism: 2
        completions: 2
        backoffLimit: 0
        #suspend: true #dws
        template:
          metadata:
            annotations:
              #iam.gke.io/gcp-service-account: "${GCP_SERVICE_ACCOUNT}"
              gke-gcsfuse/volumes: "true"
              
          spec:
            hostNetwork: true
            dnsPolicy: ClusterFirstWithHostNet
            nodeSelector:
              #cloud.google.com/gke-nodepool:  a3-ultragpu-8g-a3-ultragpu-pool #dws
              cloud.google.com/gke-accelerator: nvidia-h100-mega-80gb
            tolerations:
            #- key: cloud.google.com/gke-queued
            #  effect: NoSchedule
            # value: "true"
            - key: nvidia.com/gpu
              operator: Exists
              effect: NoSchedule
            restartPolicy: Never
            serviceAccountName: storage-access
            volumes:
            - name: libraries
              hostPath:
                path: /home/kubernetes/bin/nvidia
            - name: sys
              hostPath:
               path: /sys
            - name: proc-sys
              hostPath:
               path: /proc/sys
            - name: aperture-devices
              hostPath:
               path: /dev/aperture_devices
            - name: dshm
              emptyDir:
                medium: Memory
                sizeLimit: 256Gi
            - name: gcs-fuse-csi-ephemeral-data
              csi:
                driver: gcsfuse.csi.storage.gke.io
                volumeAttributes:
                  bucketName: rick-mlperf
                  mountOptions: "only-dir=dp-perf,implicit-dirs"
                  fileCacheCapacity: "-1Mi"
                  fileCacheForRangeRead: "true"
                  metadataCacheTTLSeconds: "-1"
                  metadataNegativeCacheTTLSeconds: "0"
                  metadataStatCacheCapacity: "-1Mi"
                  metadataTypeCacheCapacity: "-1Mi"
            - name: gcs-fuse-csi-ephemeral-model
              csi:
                driver: gcsfuse.csi.storage.gke.io
                volumeAttributes:
                  bucketName: rick-mlperf
                  mountOptions: "only-dir=meta-llama--Llama-2-70B-hf,implicit-dirs"
                  fileCacheCapacity: "-1Mi"
                  fileCacheForRangeRead: "true"
                  metadataCacheTTLSeconds: "-1"
                  metadataNegativeCacheTTLSeconds: "0"
                  metadataStatCacheCapacity: "-1Mi"
                  metadataTypeCacheCapacity: "-1Mi"
            - name: gcs-fuse-writing
              csi:
                driver: gcsfuse.csi.storage.gke.io
                volumeAttributes:
                  bucketName: rick-mlperf
                  mountOptions: "only-dir=dp-mlperf/output,implicit-dirs"
                  fileCacheCapacity: "-1Mi"
                  fileCacheForRangeRead: "true"
                  metadataCacheTTLSeconds: "-1"
                  metadataNegativeCacheTTLSeconds: "0"
                  metadataStatCacheCapacity: "-1Mi"
                  metadataTypeCacheCapacity: "-1Mi"
            - name: local-checkpoints-storage
              emptyDir: {}
            containers:
            - name: tcpxo-daemon
              image: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpxo/tcpgpudmarxd-dev:v1.0.14
              imagePullPolicy: Always
              command: ["/bin/sh", "-c"]
              args:
                - |
                  set -ex
                  chmod 755 /fts/entrypoint_rxdm_container.sh
                  /fts/entrypoint_rxdm_container.sh --num_hops=2 --num_nics=8 --uid= --alsologtostderr
              securityContext:
                privileged: true
              volumeMounts:
               - name: libraries
                 mountPath: /usr/local/nvidia
               - name: sys
                 mountPath: /hostsysfs
               - name: proc-sys
                 mountPath: /hostprocsysfs
              env:
                - name: LD_LIBRARY_PATH
                  value: /usr/local/nvidia/lib64

            - name: worker
              image: us-east1-docker.pkg.dev/gpu-launchpad-playground/gke-llm/dp-repro:latest
              imagePullPolicy: Always
              ports:
              - containerPort: 3389
              env:
              - name: HOSTNAME_PREFIX
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['jobset.sigs.k8s.io/jobset-name']
              - name: HOSTNAME_SUFFIX
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['jobset.sigs.k8s.io/replicatedjob-name']
              - name: MASTER_ADDR
                value: "train-workers-0-0.train"
              - name: MASTER_PORT
                value: "3389"
              - name: NODE_COUNT
                value: "2"
              - name: NODE_RANK
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
              - name: PYTHONUNBUFFERED
                value: "0"
              - name: NCCL_DEBUG
                value: DEBUG
              - name: LD_LIBRARY_PATH
                value: /usr/local/nvidia/lib64
              - name: NCCL_FASTRAK_LLCM_DEVICE_DIRECTORY
                value: /dev/aperture_devices
              securityContext:
                #privileged: true
              command: ["/bin/bash", "-c"]
              args:
              - |
                set -e
                rm -r -f /app/traing.sh
                rm -r -f /app/run_clm_sft_mlflow.py
                cp /mount/data/training.sh /app/training.sh
                cp /mount/data/run_clm_sft_mlflow.py /app/run_clm_sft_mlflow.py
                chmod +x /app/training.sh
                NUM_GPU_PER_NODE=8
                echo "Starting training, saving checkpoints to /mnt/local-checkpoints"
                bash /app/training.sh "\${MASTER_ADDR}" "\${MASTER_PORT}" "\${NODE_RANK}" "\${NODE_COUNT}" "$NUM_GPU_PER_NODE" "2" "128" "16"
                echo "Training finished. Copying checkpoints to persistent storage."
                # Only the rank 0 node should perform the copy to avoid race conditions.
                if [ "\${NODE_RANK}" -eq 0 ]; then
                  # rsync is often safer and more efficient than cp
                  gsutil rsync -r /mount/local-checkpoints/ gs://talkiq-data-temp-30d/"\${GCS_OUTPUT_DIR}"
                  echo "Copy complete."
                else
                  echo "Not rank 0, skipping copy."
                fi
              volumeMounts:
              - name: aperture-devices
                mountPath: /dev/aperture_devices
              - name: libraries
                mountPath: /usr/local/nvidia
              - name: gcs-fuse-csi-ephemeral-data
                mountPath: /mount/data
                readOnly: true
              - name: gcs-fuse-csi-ephemeral-model
                mountPath: /mount/base-model
                readOnly: true
              - name: gcs-fuse-writing
                mountPath: /mount/models
              - name: dshm
                mountPath: /dev/shm
              - name: local-checkpoints-storage
                mountPath: /mount/local-checkpoints
              resources:
                requests:
                  #cpu: ${NUM_CPU_CORES}
                  #memory: ${NUM_MEMORY}
                  nvidia.com/gpu: 8
                limits:
                  #cpu: ${NUM_CPU_CORES}
                  #memory: ${NUM_MEMORY}
                  nvidia.com/gpu: 8