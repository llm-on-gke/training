Starting training, saving checkpoints to /mnt/local-checkpoints
Running on node rank 0 of 2 nodes with 8 GPUs.
#!/bin/bash
export NCCL_NET=gIB
export NCCL_CROSS_NIC=0
export NCCL_NET_GDR_LEVEL=PIX
export NCCL_P2P_NET_CHUNKSIZE=131072
export NCCL_NVLS_CHUNKSIZE=524288
export NCCL_IB_ADAPTIVE_ROUTING=1
export NCCL_IB_QPS_PER_CONNECTION=4
export NCCL_IB_TC=52
export NCCL_IB_FIFO_TC=84

# Use A3 Ultra config as fallback option
export NCCL_TUNER_CONFIG_PATH=/usr/local/gib/configs/tuner_config_a3u.txtpb

# Check if PCIe device exist
check_pcie_device() {
  local vendor_id="$1"
  local device_id="$2"

  for device_path in /sys/bus/pci/devices/*; do
    if [[ -d "$device_path" ]]; then
      current_vendor_id=$(cat "$device_path/vendor" 2>/dev/null)
      current_device_id=$(cat "$device_path/device" 2>/dev/null)
      if [[ "${current_vendor_id#0x}" == "$vendor_id" ]] && \
        [[ "${current_device_id#0x}" == "$device_id" ]]; then
        return 0
      fi
    fi
  done

  return 1
}

# NVIDIA B200
if check_pcie_device 10de 2901; then
    export NCCL_TUNER_CONFIG_PATH="/usr/local/gib/configs/tuner_config_a4.txtpb"
    # echo "Found NVIDIA B200. Setting NCCL_TUNER_CONFIG_PATH to: $NCCL_TUNER_CONFIG_PATH"
# NVIDIA H200
elif check_pcie_device 10de 2335; then
    export NCCL_TUNER_CONFIG_PATH="/usr/local/gib/configs/tuner_config_a3u.txtpb"
    # echo "Found NVIDIA H200. Setting NCCL_TUNER_CONFIG_PATH to: $NCCL_TUNER_CONFIG_PATH"
fi

Environment variables set:
PCIDEVICE_NETWORKING_GKE_IO_NETWORKS_RDMA-2_INFO={"0000:98:00.0":{"generic":{"deviceID":"0000:98:00.0"},"rdma":{"rdma_cm":"/dev/infiniband/rdma_cm","uverbs":"/dev/infiniband/uverbs2"}}}
PCIDEVICE_NETWORKING_GKE_IO_NETWORKS_GVNIC-1=0000:c0:14.0
PCIDEVICE_NETWORKING_GKE_IO_NETWORKS_RDMA-6_INFO={"0000:cd:00.0":{"generic":{"deviceID":"0000:cd:00.0"},"rdma":{"rdma_cm":"/dev/infiniband/rdma_cm","uverbs":"/dev/infiniband/uverbs6"}}}
PCIDEVICE_NETWORKING_GKE_IO_NETWORKS_GVNIC-1_INFO={"0000:c0:14.0":{"generic":{"deviceID":"0000:c0:14.0"}}}
PCIDEVICE_NETWORKING_GKE_IO_NETWORKS_RDMA-3_INFO={"0000:99:00.0":{"generic":{"deviceID":"0000:99:00.0"},"rdma":{"rdma_cm":"/dev/infiniband/rdma_cm","uverbs":"/dev/infiniband/uverbs3"}}}
PCIDEVICE_NETWORKING_GKE_IO_NETWORKS_RDMA-7_INFO={"0000:ce:00.0":{"generic":{"deviceID":"0000:ce:00.0"},"rdma":{"rdma_cm":"/dev/infiniband/rdma_cm","uverbs":"/dev/infiniband/uverbs7"}}}
PCIDEVICE_NETWORKING_GKE_IO_NETWORKS_RDMA-4_INFO={"0000:c6:00.0":{"generic":{"deviceID":"0000:c6:00.0"},"rdma":{"rdma_cm":"/dev/infiniband/rdma_cm","uverbs":"/dev/infiniband/uverbs4"}}}
PCIDEVICE_NETWORKING_GKE_IO_NETWORKS_RDMA-1_INFO={"0000:92:00.0":{"generic":{"deviceID":"0000:92:00.0"},"rdma":{"rdma_cm":"/dev/infiniband/rdma_cm","uverbs":"/dev/infiniband/uverbs1"}}}
PCIDEVICE_NETWORKING_GKE_IO_NETWORKS_RDMA-5_INFO={"0000:c7:00.0":{"generic":{"deviceID":"0000:c7:00.0"},"rdma":{"rdma_cm":"/dev/infiniband/rdma_cm","uverbs":"/dev/infiniband/uverbs5"}}}
PCIDEVICE_NETWORKING_GKE_IO_NETWORKS_RDMA-5=0000:c7:00.0
PCIDEVICE_NETWORKING_GKE_IO_NETWORKS_RDMA-4=0000:c6:00.0
PCIDEVICE_NETWORKING_GKE_IO_NETWORKS_RDMA-7=0000:ce:00.0
PCIDEVICE_NETWORKING_GKE_IO_NETWORKS_RDMA-6=0000:cd:00.0
PCIDEVICE_NETWORKING_GKE_IO_NETWORKS_RDMA-0_INFO={"0000:91:00.0":{"generic":{"deviceID":"0000:91:00.0"},"rdma":{"rdma_cm":"/dev/infiniband/rdma_cm","uverbs":"/dev/infiniband/uverbs0"}}}
PCIDEVICE_NETWORKING_GKE_IO_NETWORKS_RDMA-1=0000:92:00.0
PCIDEVICE_NETWORKING_GKE_IO_NETWORKS_RDMA-0=0000:91:00.0
PCIDEVICE_NETWORKING_GKE_IO_NETWORKS_RDMA-3=0000:99:00.0
PCIDEVICE_NETWORKING_GKE_IO_NETWORKS_RDMA-2=0000:98:00.0
NV_LIBCUBLAS_VERSION=12.6.4.1-1
NVIDIA_VISIBLE_DEVICES=all
KUBERNETES_SERVICE_PORT_HTTPS=443
NCCL_IB_TC=52
NODE_COUNT=2
NV_NVML_DEV_VERSION=12.6.77-1
NV_CUDNN_PACKAGE_NAME=libcudnn9-cuda-12
KUBERNETES_SERVICE_PORT=443
PYTHONUNBUFFERED=0
NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.23.4-1+cuda12.6
NV_LIBNCCL_DEV_PACKAGE_VERSION=2.23.4-1
HOSTNAME=train-workers-0-0
NCCL_IB_FIFO_TC=84
MASTER_PORT=3389
NVIDIA_REQUIRE_CUDA=cuda>=12.6 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551
NCCL_NVLS_CHUNKSIZE=524288
NV_LIBCUBLAS_DEV_PACKAGE=libcublas-dev-12-6=12.6.4.1-1
NV_NVTX_VERSION=12.6.77-1
NV_CUDA_CUDART_DEV_VERSION=12.6.77-1
NV_LIBCUSPARSE_VERSION=12.5.4.2-1
NV_LIBNPP_VERSION=12.3.1.54-1
NCCL_VERSION=2.23.4-1
NCCL_NET_GDR_LEVEL=PIX
PWD=/app
JOB_COMPLETION_INDEX=0
NV_CUDNN_PACKAGE=libcudnn9-cuda-12=9.5.1.17-1
NVIDIA_DRIVER_CAPABILITIES=compute,utility
NV_NVPROF_DEV_PACKAGE=cuda-nvprof-12-6=12.6.80-1
NV_LIBNPP_PACKAGE=libnpp-12-6=12.3.1.54-1
NCCL_DEBUG=DEBUG
NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
NCCL_NET=gIB
NV_LIBCUBLAS_DEV_VERSION=12.6.4.1-1
NVIDIA_PRODUCT_NAME=CUDA
NV_LIBCUBLAS_DEV_PACKAGE_NAME=libcublas-dev-12-6
NV_CUDA_CUDART_VERSION=12.6.77-1
HOME=/root
KUBERNETES_PORT_443_TCP=tcp://10.0.32.1:443
CUDA_VERSION=12.6.3
NV_LIBCUBLAS_PACKAGE=libcublas-12-6=12.6.4.1-1
NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=cuda-nsight-compute-12-6=12.6.3-1
HOSTNAME_PREFIX=train
NV_LIBNPP_DEV_PACKAGE=libnpp-dev-12-6=12.3.1.54-1
NV_LIBCUBLAS_PACKAGE_NAME=libcublas-12-6
NV_LIBNPP_DEV_VERSION=12.3.1.54-1
NCCL_P2P_NET_CHUNKSIZE=131072
MASTER_ADDR=train-workers-0-0.train
NCCL_IB_QPS_PER_CONNECTION=4
NV_LIBCUSPARSE_DEV_VERSION=12.5.4.2-1
LIBRARY_PATH=/usr/local/cuda/lib64/stubs
NV_CUDNN_VERSION=9.5.1.17-1
SHLVL=1
NODE_RANK=0
NV_CUDA_LIB_VERSION=12.6.3-1
NVARCH=x86_64
KUBERNETES_PORT_443_TCP_PROTO=tcp
NV_CUDNN_PACKAGE_DEV=libcudnn9-dev-cuda-12=9.5.1.17-1
KUBERNETES_PORT_443_TCP_ADDR=10.0.32.1
NV_LIBNCCL_PACKAGE=libnccl2=2.23.4-1+cuda12.6
LD_LIBRARY_PATH=/usr/local/nvidia/lib64
NV_CUDA_NSIGHT_COMPUTE_VERSION=12.6.3-1
KUBERNETES_SERVICE_HOST=10.0.32.1
NV_NVPROF_VERSION=12.6.80-1
KUBERNETES_PORT=tcp://10.0.32.1:443
KUBERNETES_PORT_443_TCP_PORT=443
HOSTNAME_SUFFIX=workers
PATH=/root/.local/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
NV_LIBNCCL_PACKAGE_NAME=libnccl2
NV_LIBNCCL_PACKAGE_VERSION=2.23.4-1
NCCL_TUNER_CONFIG_PATH=/usr/local/gib/configs/tuner_config_a3u.txtpb
NCCL_IB_ADAPTIVE_ROUTING=1
NCCL_CROSS_NIC=0
_=/usr/bin/env

DeepSpeed environment variables:
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export NCCL_DEBUG="INFO"
export NCCL_IB_TC=52
export NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.23.4-1+cuda12.6
export NV_LIBNCCL_DEV_PACKAGE_VERSION=2.23.4-1
export NCCL_IB_FIFO_TC=84
export NCCL_NVLS_CHUNKSIZE=524288
export NCCL_VERSION=2.23.4-1
export NCCL_NET_GDR_LEVEL=PIX
export NCCL_DEBUG=DEBUG
export NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
export NCCL_NET=gIB
export NCCL_P2P_NET_CHUNKSIZE=131072
export NCCL_IB_QPS_PER_CONNECTION=4
export NV_LIBNCCL_PACKAGE=libnccl2=2.23.4-1+cuda12.6
export NV_LIBNCCL_PACKAGE_NAME=libnccl2
export NV_LIBNCCL_PACKAGE_VERSION=2.23.4-1
export NCCL_TUNER_CONFIG_PATH=/usr/local/gib/configs/tuner_config_a3u.txtpb
export NCCL_IB_ADAPTIVE_ROUTING=1
export NCCL_CROSS_NIC=0

Deepspeed hostfile with the following content:
# This file is automatically generated by the training script.
# It contains the list of nodes and their ranks.
# Format: <node_name> slots=<num_slots>
train-workers-0-0.train slots=8
train-workers-0-1.train slots=8

Listing directory contents.
ls -l /mount/data
total 314729
-rw-r--r-- 1 root root 290440042 Aug 24 13:08 10K.csv
-rw-r--r-- 1 root root  31391388 Aug 24 13:08 1K.csv
-rw-r--r-- 1 root root      1852 Aug 21 19:38 Dockerfile.txt
-rw-r--r-- 1 root root    358053 Aug 21 23:16 poetry.lock
-rw-r--r-- 1 root root      1010 Aug 21 23:16 pyproject.toml
-rw-r--r-- 1 root root     38315 Aug 24 12:43 run_clm_sft_mlflow-original.py
-rw-r--r-- 1 root root     38316 Aug 24 12:43 run_clm_sft_mlflow.py
-rw-r--r-- 1 root root      5873 Aug 24 12:28 training-original.sh
-rw-r--r-- 1 root root      5922 Aug 24 13:01 training.sh

ls -l /mount/base-model
total 15693139
-rw-r--r-- 1 root root       7627 Aug 23 02:07 LICENSE
-rw-r--r-- 1 root root      40883 Aug 23 02:07 README.md
-rw-r--r-- 1 root root       4691 Aug 23 02:07 USE_POLICY.md
-rw-r--r-- 1 root root        826 Aug 23 02:07 config.json
-rw-r--r-- 1 root root        185 Aug 23 02:07 generation_config.json
-rw-r--r-- 1 root root 4976698672 Aug 23 02:09 model-00001-of-00004.safetensors
-rw-r--r-- 1 root root 4999802720 Aug 23 02:08 model-00002-of-00004.safetensors
-rw-r--r-- 1 root root 4915916176 Aug 23 02:09 model-00003-of-00004.safetensors
-rw-r--r-- 1 root root 1168138808 Aug 23 02:08 model-00004-of-00004.safetensors
-rw-r--r-- 1 root root      23950 Aug 23 02:07 model.safetensors.index.json
-rw-r--r-- 1 root root         73 Aug 23 02:07 special_tokens_map.json
-rw-r--r-- 1 root root    9085658 Aug 23 02:07 tokenizer.json
-rw-r--r-- 1 root root      50500 Aug 23 02:07 tokenizer_config.json


Starting training using:
- master node address train-workers-0-0.train
- port 3389
- node rank 0
- total nodes 2
- GPUs per node 8
- batch size per GPU 8
- and accumulation batch size 1024.

Skipping virtualenv creation, as specified in config file.
Torch version: 2.6.0+cu124
CUDA available: True
NCCL version: (2, 21, 5)
Skipping virtualenv creation, as specified in config file.
[2025-08-24 17:36:07,285] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
df: /root/.triton/autotune: No such file or directory
[2025-08-24 17:36:15,641] [INFO] [runner.py:607:main] cmd = /usr/local/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJ0cmFpbi13b3JrZXJzLTAtMC50cmFpbiI6IFswLCAxLCAyLCAzLCA0LCA1LCA2LCA3XSwgInRyYWluLXdvcmtlcnMtMC0xLnRyYWluIjogWzAsIDEsIDIsIDMsIDQsIDUsIDYsIDddfQ== --master_addr=train-workers-0-0.train --master_port=3389 --node_rank=0 --enable_each_rank_log=None run_clm_sft_mlflow.py --report_to mlflow --attn_implementation flash_attention_2 --bf16 --torch_dtype bfloat16 --use_auth_token True --logging_strategy steps --logging_steps 1 --save_strategy steps --save_steps 800 --save_total_limit 10 --num_train_epochs 1 --learning_rate 2e-6 --warmup_steps 400 --model_name_or_path /mount/base-model --max_source_length 4000 --max_target_length 2000 --do_train --train_file /mount/data/10K.csv --validation_file /mount/data/1K.csv --text_column hinted_prompt --summary_column response --output_dir /mount/local-checkpoints --overwrite_output_dir --per_device_train_batch_size 8 --gradient_accumulation_steps 8 --eval_strategy no --per_device_eval_batch_size 1 --ddp_backend nccl --ddp_timeout 36000 --gradient_checkpointing --preprocessing_num_workers 16 --deepspeed /app/ds_config.json
[2025-08-24 17:36:17,017] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-24 17:36:19,928] [INFO] [launch.py:139:main] 0 NCCL_IB_TC=52
[2025-08-24 17:36:19,928] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.23.4-1+cuda12.6
[2025-08-24 17:36:19,928] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.23.4-1
[2025-08-24 17:36:19,928] [INFO] [launch.py:139:main] 0 NCCL_IB_FIFO_TC=84
[2025-08-24 17:36:19,928] [INFO] [launch.py:139:main] 0 NCCL_NVLS_CHUNKSIZE=524288
[2025-08-24 17:36:19,928] [INFO] [launch.py:139:main] 0 NCCL_VERSION=2.23.4-1
[2025-08-24 17:36:19,928] [INFO] [launch.py:139:main] 0 NCCL_NET_GDR_LEVEL=PIX
[2025-08-24 17:36:19,928] [INFO] [launch.py:139:main] 0 NCCL_DEBUG=DEBUG
[2025-08-24 17:36:19,928] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2025-08-24 17:36:19,928] [INFO] [launch.py:139:main] 0 NCCL_NET=gIB
[2025-08-24 17:36:19,928] [INFO] [launch.py:139:main] 0 NCCL_P2P_NET_CHUNKSIZE=131072
[2025-08-24 17:36:19,928] [INFO] [launch.py:139:main] 0 NCCL_IB_QPS_PER_CONNECTION=4
[2025-08-24 17:36:19,928] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.23.4-1+cuda12.6
[2025-08-24 17:36:19,928] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2025-08-24 17:36:19,928] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.23.4-1
[2025-08-24 17:36:19,928] [INFO] [launch.py:139:main] 0 NCCL_TUNER_CONFIG_PATH=/usr/local/gib/configs/tuner_config_a3u.txtpb
[2025-08-24 17:36:19,928] [INFO] [launch.py:139:main] 0 NCCL_IB_ADAPTIVE_ROUTING=1
[2025-08-24 17:36:19,928] [INFO] [launch.py:139:main] 0 NCCL_CROSS_NIC=0
[2025-08-24 17:36:19,928] [INFO] [launch.py:146:main] WORLD INFO DICT: {'train-workers-0-0.train': [0, 1, 2, 3, 4, 5, 6, 7], 'train-workers-0-1.train': [0, 1, 2, 3, 4, 5, 6, 7]}
[2025-08-24 17:36:19,928] [INFO] [launch.py:152:main] nnodes=2, num_local_procs=8, node_rank=0
[2025-08-24 17:36:19,928] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'train-workers-0-0.train': [0, 1, 2, 3, 4, 5, 6, 7], 'train-workers-0-1.train': [8, 9, 10, 11, 12, 13, 14, 15]})
[2025-08-24 17:36:19,928] [INFO] [launch.py:164:main] dist_world_size=16
[2025-08-24 17:36:19,928] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2025-08-24 17:36:19,929] [INFO] [launch.py:256:main] process 1756 spawned with command: ['/usr/local/bin/python3.11', '-u', 'run_clm_sft_mlflow.py', '--local_rank=0', '--report_to', 'mlflow', '--attn_implementation', 'flash_attention_2', '--bf16', '--torch_dtype', 'bfloat16', '--use_auth_token', 'True', '--logging_strategy', 'steps', '--logging_steps', '1', '--save_strategy', 'steps', '--save_steps', '800', '--save_total_limit', '10', '--num_train_epochs', '1', '--learning_rate', '2e-6', '--warmup_steps', '400', '--model_name_or_path', '/mount/base-model', '--max_source_length', '4000', '--max_target_length', '2000', '--do_train', '--train_file', '/mount/data/10K.csv', '--validation_file', '/mount/data/1K.csv', '--text_column', 'hinted_prompt', '--summary_column', 'response', '--output_dir', '/mount/local-checkpoints', '--overwrite_output_dir', '--per_device_train_batch_size', '8', '--gradient_accumulation_steps', '8', '--eval_strategy', 'no', '--per_device_eval_batch_size', '1', '--ddp_backend', 'nccl', '--ddp_timeout', '36000', '--gradient_checkpointing', '--preprocessing_num_workers', '16', '--deepspeed', '/app/ds_config.json']
[2025-08-24 17:36:19,929] [INFO] [launch.py:256:main] process 1757 spawned with command: ['/usr/local/bin/python3.11', '-u', 'run_clm_sft_mlflow.py', '--local_rank=1', '--report_to', 'mlflow', '--attn_implementation', 'flash_attention_2', '--bf16', '--torch_dtype', 'bfloat16', '--use_auth_token', 'True', '--logging_strategy', 'steps', '--logging_steps', '1', '--save_strategy', 'steps', '--save_steps', '800', '--save_total_limit', '10', '--num_train_epochs', '1', '--learning_rate', '2e-6', '--warmup_steps', '400', '--model_name_or_path', '/mount/base-model', '--max_source_length', '4000', '--max_target_length', '2000', '--do_train', '--train_file', '/mount/data/10K.csv', '--validation_file', '/mount/data/1K.csv', '--text_column', 'hinted_prompt', '--summary_column', 'response', '--output_dir', '/mount/local-checkpoints', '--overwrite_output_dir', '--per_device_train_batch_size', '8', '--gradient_accumulation_steps', '8', '--eval_strategy', 'no', '--per_device_eval_batch_size', '1', '--ddp_backend', 'nccl', '--ddp_timeout', '36000', '--gradient_checkpointing', '--preprocessing_num_workers', '16', '--deepspeed', '/app/ds_config.json']
[2025-08-24 17:36:19,930] [INFO] [launch.py:256:main] process 1758 spawned with command: ['/usr/local/bin/python3.11', '-u', 'run_clm_sft_mlflow.py', '--local_rank=2', '--report_to', 'mlflow', '--attn_implementation', 'flash_attention_2', '--bf16', '--torch_dtype', 'bfloat16', '--use_auth_token', 'True', '--logging_strategy', 'steps', '--logging_steps', '1', '--save_strategy', 'steps', '--save_steps', '800', '--save_total_limit', '10', '--num_train_epochs', '1', '--learning_rate', '2e-6', '--warmup_steps', '400', '--model_name_or_path', '/mount/base-model', '--max_source_length', '4000', '--max_target_length', '2000', '--do_train', '--train_file', '/mount/data/10K.csv', '--validation_file', '/mount/data/1K.csv', '--text_column', 'hinted_prompt', '--summary_column', 'response', '--output_dir', '/mount/local-checkpoints', '--overwrite_output_dir', '--per_device_train_batch_size', '8', '--gradient_accumulation_steps', '8', '--eval_strategy', 'no', '--per_device_eval_batch_size', '1', '--ddp_backend', 'nccl', '--ddp_timeout', '36000', '--gradient_checkpointing', '--preprocessing_num_workers', '16', '--deepspeed', '/app/ds_config.json']
[2025-08-24 17:36:19,930] [INFO] [launch.py:256:main] process 1759 spawned with command: ['/usr/local/bin/python3.11', '-u', 'run_clm_sft_mlflow.py', '--local_rank=3', '--report_to', 'mlflow', '--attn_implementation', 'flash_attention_2', '--bf16', '--torch_dtype', 'bfloat16', '--use_auth_token', 'True', '--logging_strategy', 'steps', '--logging_steps', '1', '--save_strategy', 'steps', '--save_steps', '800', '--save_total_limit', '10', '--num_train_epochs', '1', '--learning_rate', '2e-6', '--warmup_steps', '400', '--model_name_or_path', '/mount/base-model', '--max_source_length', '4000', '--max_target_length', '2000', '--do_train', '--train_file', '/mount/data/10K.csv', '--validation_file', '/mount/data/1K.csv', '--text_column', 'hinted_prompt', '--summary_column', 'response', '--output_dir', '/mount/local-checkpoints', '--overwrite_output_dir', '--per_device_train_batch_size', '8', '--gradient_accumulation_steps', '8', '--eval_strategy', 'no', '--per_device_eval_batch_size', '1', '--ddp_backend', 'nccl', '--ddp_timeout', '36000', '--gradient_checkpointing', '--preprocessing_num_workers', '16', '--deepspeed', '/app/ds_config.json']
[2025-08-24 17:36:19,931] [INFO] [launch.py:256:main] process 1760 spawned with command: ['/usr/local/bin/python3.11', '-u', 'run_clm_sft_mlflow.py', '--local_rank=4', '--report_to', 'mlflow', '--attn_implementation', 'flash_attention_2', '--bf16', '--torch_dtype', 'bfloat16', '--use_auth_token', 'True', '--logging_strategy', 'steps', '--logging_steps', '1', '--save_strategy', 'steps', '--save_steps', '800', '--save_total_limit', '10', '--num_train_epochs', '1', '--learning_rate', '2e-6', '--warmup_steps', '400', '--model_name_or_path', '/mount/base-model', '--max_source_length', '4000', '--max_target_length', '2000', '--do_train', '--train_file', '/mount/data/10K.csv', '--validation_file', '/mount/data/1K.csv', '--text_column', 'hinted_prompt', '--summary_column', 'response', '--output_dir', '/mount/local-checkpoints', '--overwrite_output_dir', '--per_device_train_batch_size', '8', '--gradient_accumulation_steps', '8', '--eval_strategy', 'no', '--per_device_eval_batch_size', '1', '--ddp_backend', 'nccl', '--ddp_timeout', '36000', '--gradient_checkpointing', '--preprocessing_num_workers', '16', '--deepspeed', '/app/ds_config.json']
[2025-08-24 17:36:19,931] [INFO] [launch.py:256:main] process 1761 spawned with command: ['/usr/local/bin/python3.11', '-u', 'run_clm_sft_mlflow.py', '--local_rank=5', '--report_to', 'mlflow', '--attn_implementation', 'flash_attention_2', '--bf16', '--torch_dtype', 'bfloat16', '--use_auth_token', 'True', '--logging_strategy', 'steps', '--logging_steps', '1', '--save_strategy', 'steps', '--save_steps', '800', '--save_total_limit', '10', '--num_train_epochs', '1', '--learning_rate', '2e-6', '--warmup_steps', '400', '--model_name_or_path', '/mount/base-model', '--max_source_length', '4000', '--max_target_length', '2000', '--do_train', '--train_file', '/mount/data/10K.csv', '--validation_file', '/mount/data/1K.csv', '--text_column', 'hinted_prompt', '--summary_column', 'response', '--output_dir', '/mount/local-checkpoints', '--overwrite_output_dir', '--per_device_train_batch_size', '8', '--gradient_accumulation_steps', '8', '--eval_strategy', 'no', '--per_device_eval_batch_size', '1', '--ddp_backend', 'nccl', '--ddp_timeout', '36000', '--gradient_checkpointing', '--preprocessing_num_workers', '16', '--deepspeed', '/app/ds_config.json']
[2025-08-24 17:36:19,931] [INFO] [launch.py:256:main] process 1762 spawned with command: ['/usr/local/bin/python3.11', '-u', 'run_clm_sft_mlflow.py', '--local_rank=6', '--report_to', 'mlflow', '--attn_implementation', 'flash_attention_2', '--bf16', '--torch_dtype', 'bfloat16', '--use_auth_token', 'True', '--logging_strategy', 'steps', '--logging_steps', '1', '--save_strategy', 'steps', '--save_steps', '800', '--save_total_limit', '10', '--num_train_epochs', '1', '--learning_rate', '2e-6', '--warmup_steps', '400', '--model_name_or_path', '/mount/base-model', '--max_source_length', '4000', '--max_target_length', '2000', '--do_train', '--train_file', '/mount/data/10K.csv', '--validation_file', '/mount/data/1K.csv', '--text_column', 'hinted_prompt', '--summary_column', 'response', '--output_dir', '/mount/local-checkpoints', '--overwrite_output_dir', '--per_device_train_batch_size', '8', '--gradient_accumulation_steps', '8', '--eval_strategy', 'no', '--per_device_eval_batch_size', '1', '--ddp_backend', 'nccl', '--ddp_timeout', '36000', '--gradient_checkpointing', '--preprocessing_num_workers', '16', '--deepspeed', '/app/ds_config.json']
[2025-08-24 17:36:19,932] [INFO] [launch.py:256:main] process 1763 spawned with command: ['/usr/local/bin/python3.11', '-u', 'run_clm_sft_mlflow.py', '--local_rank=7', '--report_to', 'mlflow', '--attn_implementation', 'flash_attention_2', '--bf16', '--torch_dtype', 'bfloat16', '--use_auth_token', 'True', '--logging_strategy', 'steps', '--logging_steps', '1', '--save_strategy', 'steps', '--save_steps', '800', '--save_total_limit', '10', '--num_train_epochs', '1', '--learning_rate', '2e-6', '--warmup_steps', '400', '--model_name_or_path', '/mount/base-model', '--max_source_length', '4000', '--max_target_length', '2000', '--do_train', '--train_file', '/mount/data/10K.csv', '--validation_file', '/mount/data/1K.csv', '--text_column', 'hinted_prompt', '--summary_column', 'response', '--output_dir', '/mount/local-checkpoints', '--overwrite_output_dir', '--per_device_train_batch_size', '8', '--gradient_accumulation_steps', '8', '--eval_strategy', 'no', '--per_device_eval_batch_size', '1', '--ddp_backend', 'nccl', '--ddp_timeout', '36000', '--gradient_checkpointing', '--preprocessing_num_workers', '16', '--deepspeed', '/app/ds_config.json']
[2025-08-24 17:36:25,480] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-24 17:36:25,604] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-24 17:36:25,646] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-24 17:36:25,657] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-24 17:36:25,734] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-24 17:36:25,742] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-24 17:36:25,758] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-24 17:36:25,759] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-24 17:36:27,220] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-08-24 17:36:27,565] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-08-24 17:36:27,565] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-08-24 17:36:27,583] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-08-24 17:36:27,706] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-08-24 17:36:27,739] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-08-24 17:36:27,754] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-08-24 17:36:27,756] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-08-24 17:36:27,758] [INFO] [comm.py:652:init_distributed] cdb=None
2025-08-24 17:36:28,677 - __main__ - WARNING - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
2025-08-24 17:36:28,677 - __main__ - WARNING - Backend: nccl
2025-08-24 17:36:28,677 - __main__ - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=nccl,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=36000,
debug=[],
deepspeed=/app/ds_config.json,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/mount/local-checkpoints/runs/Aug24_17-36-27_train-workers-0-0,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/mount/local-checkpoints,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['mlflow'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/mount/local-checkpoints,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=800,
save_strategy=SaveStrategy.STEPS,
save_total_limit=10,
seed=42,
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=400,
weight_decay=0.0,
)
/usr/local/lib/python3.11/site-packages/datasets/load.py:2566: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
2025-08-24 17:36:28,924 - __main__ - WARNING - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
2025-08-24 17:36:28,925 - __main__ - WARNING - Backend: nccl
2025-08-24 17:36:28,926 - __main__ - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=nccl,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=36000,
debug=[],
deepspeed=/app/ds_config.json,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/mount/local-checkpoints/runs/Aug24_17-36-27_train-workers-0-0,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/mount/local-checkpoints,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['mlflow'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/mount/local-checkpoints,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=800,
save_strategy=SaveStrategy.STEPS,
save_total_limit=10,
seed=42,
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=400,
weight_decay=0.0,
)
/usr/local/lib/python3.11/site-packages/datasets/load.py:2566: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
2025-08-24 17:36:28,940 - __main__ - WARNING - Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: False
2025-08-24 17:36:28,941 - __main__ - WARNING - Backend: nccl
2025-08-24 17:36:28,941 - __main__ - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=nccl,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=36000,
debug=[],
deepspeed=/app/ds_config.json,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=6,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/mount/local-checkpoints/runs/Aug24_17-36-27_train-workers-0-0,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/mount/local-checkpoints,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['mlflow'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/mount/local-checkpoints,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=800,
save_strategy=SaveStrategy.STEPS,
save_total_limit=10,
seed=42,
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=400,
weight_decay=0.0,
)
/usr/local/lib/python3.11/site-packages/datasets/load.py:2566: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
2025-08-24 17:36:28,969 - __main__ - WARNING - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
2025-08-24 17:36:28,969 - __main__ - WARNING - Backend: nccl
2025-08-24 17:36:28,969 - __main__ - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=nccl,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=36000,
debug=[],
deepspeed=/app/ds_config.json,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/mount/local-checkpoints/runs/Aug24_17-36-27_train-workers-0-0,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/mount/local-checkpoints,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['mlflow'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/mount/local-checkpoints,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=800,
save_strategy=SaveStrategy.STEPS,
save_total_limit=10,
seed=42,
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=400,
weight_decay=0.0,
)
2025-08-24 17:36:28,970 - __main__ - WARNING - Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: False
2025-08-24 17:36:28,970 - __main__ - WARNING - Backend: nccl
2025-08-24 17:36:28,970 - __main__ - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=nccl,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=36000,
debug=[],
deepspeed=/app/ds_config.json,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=7,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/mount/local-checkpoints/runs/Aug24_17-36-27_train-workers-0-0,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/mount/local-checkpoints,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['mlflow'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/mount/local-checkpoints,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=800,
save_strategy=SaveStrategy.STEPS,
save_total_limit=10,
seed=42,
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=400,
weight_decay=0.0,
)
/usr/local/lib/python3.11/site-packages/datasets/load.py:2566: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/datasets/load.py:2566: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
2025-08-24 17:36:28,973 - __main__ - WARNING - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
2025-08-24 17:36:28,974 - __main__ - WARNING - Backend: nccl
2025-08-24 17:36:28,974 - __main__ - WARNING - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: False
2025-08-24 17:36:28,974 - __main__ - WARNING - Backend: nccl
2025-08-24 17:36:28,974 - __main__ - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=nccl,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=36000,
debug=[],
deepspeed=/app/ds_config.json,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/mount/local-checkpoints/runs/Aug24_17-36-27_train-workers-0-0,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/mount/local-checkpoints,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['mlflow'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/mount/local-checkpoints,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=800,
save_strategy=SaveStrategy.STEPS,
save_total_limit=10,
seed=42,
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=400,
weight_decay=0.0,
)
2025-08-24 17:36:28,974 - __main__ - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=nccl,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=36000,
debug=[],
deepspeed=/app/ds_config.json,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=5,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/mount/local-checkpoints/runs/Aug24_17-36-27_train-workers-0-0,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/mount/local-checkpoints,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['mlflow'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/mount/local-checkpoints,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=800,
save_strategy=SaveStrategy.STEPS,
save_total_limit=10,
seed=42,
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=400,
weight_decay=0.0,
)
Using custom data configuration default-7c55f3ae8d004c8a
Loading Dataset Infos from /usr/local/lib/python3.11/site-packages/datasets/packaged_modules/csv
2025-08-24 17:36:28,976 - __main__ - WARNING - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: False
2025-08-24 17:36:28,976 - __main__ - WARNING - Backend: nccl
2025-08-24 17:36:28,976 - __main__ - INFO - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=nccl,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=36000,
debug=[],
deepspeed=/app/ds_config.json,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.NO,
eval_use_gather_object=False,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=4,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/mount/local-checkpoints/runs/Aug24_17-36-27_train-workers-0-0,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=/mount/local-checkpoints,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['mlflow'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=/mount/local-checkpoints,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=800,
save_strategy=SaveStrategy.STEPS,
save_total_limit=10,
seed=42,
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=400,
weight_decay=0.0,
)
/usr/local/lib/python3.11/site-packages/datasets/load.py:2566: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/datasets/load.py:2566: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/datasets/load.py:2566: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52)
Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52...
Downloading took 0.0 min
Checksum Computation took 0.0 min
Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 10000 examples [00:00, 25959.30 examples/s]Generating train split: 20000 examples [00:00, 26042.80 examples/s]Generating train split: 30000 examples [00:01, 28164.55 examples/s]Generating train split: 40000 examples [00:01, 29103.76 examples/s]Generating train split: 50000 examples [00:02, 13402.72 examples/s]Generating train split: 60000 examples [00:03, 16593.43 examples/s]Generating train split: 70000 examples [00:03, 19900.81 examples/s]Generating train split: 78160 examples [00:03, 22159.52 examples/s]Generating train split: 78160 examples [00:03, 20809.63 examples/s]
Generating validation split
Generating validation split: 0 examples [00:00, ? examples/s]Generating validation split: 8682 examples [00:00, 30133.62 examples/s]Generating validation split: 8682 examples [00:00, 29839.76 examples/s]
Unable to verify splits sizes.
Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52. Subsequent calls will reuse this data.
/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1099: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1099: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1099: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1099: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1099: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1099: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1099: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1099: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
[INFO|configuration_utils.py:691] 2025-08-24 17:36:35,920 >> loading configuration file /mount/base-model/config.json
[INFO|configuration_utils.py:765] 2025-08-24 17:36:35,921 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "vocab_size": 128256
}

/usr/local/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
[INFO|tokenization_utils_base.py:2058] 2025-08-24 17:36:37,971 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2058] 2025-08-24 17:36:37,971 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2058] 2025-08-24 17:36:37,971 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2058] 2025-08-24 17:36:37,971 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2058] 2025-08-24 17:36:37,971 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2058] 2025-08-24 17:36:37,971 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2323] 2025-08-24 17:36:38,457 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/usr/local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
[INFO|modeling_utils.py:1121] 2025-08-24 17:36:38,842 >> loading weights file /mount/base-model/model.safetensors.index.json
[INFO|modeling_utils.py:2167] 2025-08-24 17:36:38,960 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:3726] 2025-08-24 17:36:38,960 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[2025-08-24 17:36:38,960] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 16
[2025-08-24 17:36:38,960] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 16
hpZeRO group size: 8
[WARNING|logging.py:328] 2025-08-24 17:36:38,962 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[WARNING|logging.py:328] 2025-08-24 17:36:38,963 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[INFO|configuration_utils.py:1142] 2025-08-24 17:36:38,966 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

/usr/local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/usr/local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
[2025-08-24 17:36:39,346] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 16
[WARNING|logging.py:328] 2025-08-24 17:36:39,349 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2025-08-24 17:36:39,452] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 16
[WARNING|logging.py:328] 2025-08-24 17:36:39,455 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2025-08-24 17:36:39,860] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 16
[WARNING|logging.py:328] 2025-08-24 17:36:39,862 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2025-08-24 17:36:39,964] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 16
[WARNING|logging.py:328] 2025-08-24 17:36:39,967 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2025-08-24 17:36:40,357] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 16
[WARNING|logging.py:328] 2025-08-24 17:36:40,360 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2025-08-24 17:36:40,459] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 16
[WARNING|logging.py:328] 2025-08-24 17:36:40,462 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2025-08-24 17:36:43,587] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 8.03B
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:12<00:37, 12.59s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:12<00:37, 12.59s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:12<00:37, 12.59s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:12<00:37, 12.59s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:12<00:37, 12.59s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:12<00:37, 12.59s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:12<00:37, 12.59s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:15<00:45, 15.05s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:25<00:25, 12.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:25<00:25, 12.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:25<00:25, 12.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:25<00:25, 12.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:25<00:25, 12.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:25<00:25, 12.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:25<00:25, 12.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:28<00:27, 13.93s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.81s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.81s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.81s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.81s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.81s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.81s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00,  8.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00, 10.02s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00,  8.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00, 10.02s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00,  8.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00,  8.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00, 10.02s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00, 10.02s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00,  8.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00, 10.02s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00,  8.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00,  8.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00, 10.02s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00, 10.02s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:40<00:13, 13.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:44<00:00,  9.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:44<00:00, 11.11s/it]
[INFO|modeling_utils.py:4930] 2025-08-24 17:37:28,052 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4938] 2025-08-24 17:37:28,052 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /mount/base-model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1095] 2025-08-24 17:37:28,054 >> loading configuration file /mount/base-model/generation_config.json
[INFO|configuration_utils.py:1142] 2025-08-24 17:37:28,054 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

2025-08-24 17:37:28,133 - __main__ - WARNING - >>> enable gradient_checkpointing
2025-08-24 17:37:28,136 - __main__ - WARNING - >>> enable gradient_checkpointing
2025-08-24 17:37:28,136 - __main__ - WARNING - >>> enable gradient_checkpointing
2025-08-24 17:37:28,137 - __main__ - WARNING - >>> enable gradient_checkpointing
2025-08-24 17:37:28,137 - __main__ - WARNING - >>> enable gradient_checkpointing
2025-08-24 17:37:28,140 - __main__ - WARNING - >>> enable gradient_checkpointing
2025-08-24 17:37:28,144 - __main__ - WARNING - >>> enable gradient_checkpointing
2025-08-24 17:37:28,144 - __main__ - WARNING - >>> enable gradient_checkpointing
Process #0 will write at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00000_of_00016.arrow
Process #1 will write at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00001_of_00016.arrow
Process #2 will write at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00002_of_00016.arrow
Process #3 will write at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00003_of_00016.arrow
Process #4 will write at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00004_of_00016.arrow
Process #5 will write at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00005_of_00016.arrow
Process #6 will write at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00006_of_00016.arrow
Process #7 will write at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00007_of_00016.arrow
Process #8 will write at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00008_of_00016.arrow
Process #9 will write at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00009_of_00016.arrow
Process #10 will write at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00010_of_00016.arrow
Process #11 will write at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00011_of_00016.arrow
Process #12 will write at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00012_of_00016.arrow
Process #13 will write at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00013_of_00016.arrow
Process #14 will write at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00014_of_00016.arrow
Process #15 will write at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00015_of_00016.arrow
Spawning 16 processes
Running tokenizer on train dataset (num_proc=16):   0%|          | 0/78160 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00002_of_00016.arrow
Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00000_of_00016.arrow
Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00004_of_00016.arrow
Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00006_of_00016.arrow
Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00001_of_00016.arrow
Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00003_of_00016.arrow
Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00007_of_00016.arrow
Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00005_of_00016.arrow
Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00008_of_00016.arrow
Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00009_of_00016.arrow
Running tokenizer on train dataset (num_proc=16):   1%|▏         | 1000/78160 [00:20<25:55, 49.61 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00010_of_00016.arrow
Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00013_of_00016.arrow
Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00011_of_00016.arrow
Running tokenizer on train dataset (num_proc=16):   4%|▍         | 3000/78160 [00:20<06:37, 189.21 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00012_of_00016.arrow
Running tokenizer on train dataset (num_proc=16):   6%|▋         | 5000/78160 [00:20<03:11, 382.86 examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00015_of_00016.arrow
Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7c55f3ae8d004c8a/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-aa70139d7a6a607f_00014_of_00016.arrow
Running tokenizer on train dataset (num_proc=16):  10%|█         | 8000/78160 [00:20<01:29, 785.65 examples/s]Running tokenizer on train dataset (num_proc=16):  13%|█▎        | 10000/78160 [00:20<01:01, 1111.33 examples/s]Running tokenizer on train dataset (num_proc=16):  15%|█▌        | 12000/78160 [00:21<00:42, 1559.19 examples/s]Running tokenizer on train dataset (num_proc=16):  18%|█▊        | 14000/78160 [00:21<00:29, 2174.11 examples/s]Running tokenizer on train dataset (num_proc=16):  20%|██        | 16000/78160 [00:21<00:21, 2850.16 examples/s]Running tokenizer on train dataset (num_proc=16):  20%|██        | 16000/78160 [00:36<00:21, 2850.16 examples/s]Running tokenizer on train dataset (num_proc=16):  22%|██▏       | 17000/78160 [00:39<03:36, 282.06 examples/s] Running tokenizer on train dataset (num_proc=16):  26%|██▌       | 20000/78160 [00:39<01:58, 492.13 examples/s]Running tokenizer on train dataset (num_proc=16):  28%|██▊       | 22000/78160 [00:39<01:22, 681.98 examples/s]Running tokenizer on train dataset (num_proc=16):  32%|███▏      | 25000/78160 [00:40<00:49, 1078.38 examples/s]Running tokenizer on train dataset (num_proc=16):  35%|███▍      | 27000/78160 [00:40<00:36, 1389.04 examples/s]Running tokenizer on train dataset (num_proc=16):  37%|███▋      | 29000/78160 [00:40<00:27, 1819.39 examples/s]Running tokenizer on train dataset (num_proc=16):  41%|████      | 32000/78160 [00:41<00:18, 2530.30 examples/s]Running tokenizer on train dataset (num_proc=16):  41%|████      | 32000/78160 [00:56<00:18, 2530.30 examples/s]Running tokenizer on train dataset (num_proc=16):  42%|████▏     | 33000/78160 [00:58<02:14, 334.75 examples/s] Running tokenizer on train dataset (num_proc=16):  45%|████▍     | 35000/78160 [00:58<01:30, 475.29 examples/s]Running tokenizer on train dataset (num_proc=16):  47%|████▋     | 37000/78160 [00:58<01:01, 672.35 examples/s]Running tokenizer on train dataset (num_proc=16):  50%|████▉     | 39000/78160 [00:59<00:41, 934.13 examples/s]Running tokenizer on train dataset (num_proc=16):  52%|█████▏    | 41000/78160 [00:59<00:28, 1293.92 examples/s]Running tokenizer on train dataset (num_proc=16):  55%|█████▌    | 43000/78160 [00:59<00:20, 1677.64 examples/s]Running tokenizer on train dataset (num_proc=16):  58%|█████▊    | 45000/78160 [00:59<00:15, 2192.04 examples/s]Running tokenizer on train dataset (num_proc=16):  60%|██████    | 47000/78160 [01:00<00:10, 2915.13 examples/s]Running tokenizer on train dataset (num_proc=16):  61%|██████▏   | 48000/78160 [01:11<00:10, 2915.13 examples/s]Running tokenizer on train dataset (num_proc=16):  63%|██████▎   | 49000/78160 [01:17<01:25, 341.96 examples/s] Running tokenizer on train dataset (num_proc=16):  65%|██████▌   | 51000/78160 [01:18<00:56, 482.25 examples/s]Running tokenizer on train dataset (num_proc=16):  68%|██████▊   | 53000/78160 [01:18<00:36, 680.04 examples/s]Running tokenizer on train dataset (num_proc=16):  73%|███████▎  | 57000/78160 [01:18<00:17, 1213.19 examples/s]Running tokenizer on train dataset (num_proc=16):  75%|███████▌  | 59000/78160 [01:18<00:13, 1470.00 examples/s]Running tokenizer on train dataset (num_proc=16):  78%|███████▊  | 61000/78160 [01:19<00:08, 1920.23 examples/s]Running tokenizer on train dataset (num_proc=16):  81%|████████  | 63000/78160 [01:19<00:06, 2453.15 examples/s]Running tokenizer on train dataset (num_proc=16):  82%|████████▏ | 64000/78160 [01:31<00:05, 2453.15 examples/s]Running tokenizer on train dataset (num_proc=16):  83%|████████▎ | 64885/78160 [01:34<00:33, 399.24 examples/s] Running tokenizer on train dataset (num_proc=16):  84%|████████▍ | 65770/78160 [01:35<00:26, 464.50 examples/s]Running tokenizer on train dataset (num_proc=16):  86%|████████▋ | 67540/78160 [01:35<00:16, 658.19 examples/s]Running tokenizer on train dataset (num_proc=16):  89%|████████▊ | 69310/78160 [01:35<00:09, 915.72 examples/s]Running tokenizer on train dataset (num_proc=16):  91%|█████████ | 71080/78160 [01:35<00:05, 1267.42 examples/s]Running tokenizer on train dataset (num_proc=16):  93%|█████████▎| 72850/78160 [01:35<00:03, 1738.48 examples/s]Running tokenizer on train dataset (num_proc=16):  95%|█████████▌| 74620/78160 [01:36<00:01, 2082.82 examples/s]Running tokenizer on train dataset (num_proc=16):  99%|█████████▉| 77275/78160 [01:36<00:00, 3180.34 examples/s]Running tokenizer on train dataset (num_proc=16): 100%|██████████| 78160/78160 [01:37<00:00, 804.29 examples/s] 
Concatenating 16 shards
Downloading builder script: 0.00B [00:00, ?B/s]Downloading builder script: 4.20kB [00:00, 14.7MB/s]
/app/run_clm_sft_mlflow.py:791: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  trainer = CustomTrainer(
[INFO|trainer.py:748] 2025-08-24 17:39:06,827 >> Using auto half precision backend
/app/run_clm_sft_mlflow.py:791: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  trainer = CustomTrainer(
Downloading builder script: 0.00B [00:00, ?B/s]Downloading builder script: 4.20kB [00:00, 21.2MB/s]
/app/run_clm_sft_mlflow.py:791: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  trainer = CustomTrainer(
/app/run_clm_sft_mlflow.py:791: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  trainer = CustomTrainer(
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 8. Using DeepSpeed's value.
[2025-08-24 17:39:07,099] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.15.4, git-hash=unknown, git-branch=unknown
[2025-08-24 17:39:07,099] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 16
[2025-08-24 17:39:07,105] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...

Creating extension directory /root/.cache/torch_extensions/py311_cu124/fused_adam...Creating extension directory /root/.cache/torch_extensions/py311_cu124/fused_adam...

Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py311_cu124/fused_adam/build.ninja...
/usr/local/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Downloading builder script: 0.00B [00:00, ?B/s]Downloading builder script: 4.20kB [00:00, 20.1MB/s]
/app/run_clm_sft_mlflow.py:791: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  trainer = CustomTrainer(
Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...
Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...
Downloading builder script: 0.00B [00:00, ?B/s]Downloading builder script: 4.20kB [00:00, 18.8MB/s]
/app/run_clm_sft_mlflow.py:791: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  trainer = CustomTrainer(
Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...
Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...
Downloading builder script: 0.00B [00:00, ?B/s]Downloading builder script: 4.20kB [00:00, 12.2MB/s]
/app/run_clm_sft_mlflow.py:791: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  trainer = CustomTrainer(
/app/run_clm_sft_mlflow.py:791: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  trainer = CustomTrainer(
Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...
Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...
[1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output multi_tensor_adam.cuda.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/usr/local/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.11/site-packages/torch/include -isystem /usr/local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/site-packages/torch/include/TH -isystem /usr/local/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/local/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_90,code=sm_90 -gencode=arch=compute_90,code=compute_90 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -std=c++17 -c /usr/local/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o 
[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/usr/local/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.11/site-packages/torch/include -isystem /usr/local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/site-packages/torch/include/TH -isystem /usr/local/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/local/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /usr/local/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o 
[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/usr/local/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so
Loading extension module fused_adam...
Time to load fused_adam op: 17.577974557876587 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 14.718472957611084 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 17.52186632156372 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 17.221736192703247 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 17.622605323791504 seconds
[2025-08-24 17:39:24,731] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-08-24 17:39:24,732] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-08-24 17:39:24,738] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-08-24 17:39:24,738] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-08-24 17:39:24,738] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2025-08-24 17:39:24,738] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
Loading extension module fused_adam...
Time to load fused_adam op: 17.522582054138184 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 17.416451930999756 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 14.818413496017456 seconds
[2025-08-24 17:39:24,947] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2025-08-24 17:39:24,948] [INFO] [utils.py:782:see_memory_usage] MA 4.78 GB         Max_MA 6.73 GB         CA 4.8 GB         Max_CA 9 GB 
[2025-08-24 17:39:24,948] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 42.65 GB, percent = 1.5%
[2025-08-24 17:39:24,949] [INFO] [stage3.py:166:__init__] Reduce bucket size 16777216
[2025-08-24 17:39:24,949] [INFO] [stage3.py:167:__init__] Prefetch bucket size 1000000000
[2025-08-24 17:39:25,051] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-08-24 17:39:25,052] [INFO] [utils.py:782:see_memory_usage] MA 4.78 GB         Max_MA 4.78 GB         CA 4.8 GB         Max_CA 5 GB 
[2025-08-24 17:39:25,052] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 42.65 GB, percent = 1.5%
Parameter Offload: Total persistent parameters: 266240 in 65 params
[2025-08-24 17:39:25,179] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-08-24 17:39:25,179] [INFO] [utils.py:782:see_memory_usage] MA 3.19 GB         Max_MA 4.96 GB         CA 4.98 GB         Max_CA 5 GB 
[2025-08-24 17:39:25,179] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 42.66 GB, percent = 1.5%
[2025-08-24 17:39:25,275] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2025-08-24 17:39:25,275] [INFO] [utils.py:782:see_memory_usage] MA 3.19 GB         Max_MA 3.19 GB         CA 4.98 GB         Max_CA 5 GB 
[2025-08-24 17:39:25,275] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 42.66 GB, percent = 1.5%
[2025-08-24 17:39:26,289] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1
[2025-08-24 17:39:26,290] [INFO] [utils.py:782:see_memory_usage] MA 3.18 GB         Max_MA 3.19 GB         CA 4.87 GB         Max_CA 5 GB 
[2025-08-24 17:39:26,290] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 42.63 GB, percent = 1.5%
[2025-08-24 17:39:26,404] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2025-08-24 17:39:26,404] [INFO] [utils.py:782:see_memory_usage] MA 3.18 GB         Max_MA 3.18 GB         CA 4.87 GB         Max_CA 5 GB 
[2025-08-24 17:39:26,404] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 42.63 GB, percent = 1.5%
[2025-08-24 17:39:26,501] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2025-08-24 17:39:26,501] [INFO] [utils.py:782:see_memory_usage] MA 5.05 GB         Max_MA 5.98 GB         CA 7.68 GB         Max_CA 8 GB 
[2025-08-24 17:39:26,501] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 42.62 GB, percent = 1.5%
[2025-08-24 17:39:26,598] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-08-24 17:39:26,598] [INFO] [utils.py:782:see_memory_usage] MA 5.05 GB         Max_MA 5.05 GB         CA 7.68 GB         Max_CA 8 GB 
[2025-08-24 17:39:26,598] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 42.62 GB, percent = 1.5%
[2025-08-24 17:39:26,694] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-08-24 17:39:26,695] [INFO] [utils.py:782:see_memory_usage] MA 5.05 GB         Max_MA 6.92 GB         CA 9.55 GB         Max_CA 10 GB 
[2025-08-24 17:39:26,695] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 42.62 GB, percent = 1.5%
[2025-08-24 17:39:26,695] [INFO] [stage3.py:521:_setup_for_real_optimizer] optimizer state initialized
[2025-08-24 17:39:26,798] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-08-24 17:39:26,798] [WARNING] [lr_schedules.py:759:__init__] total_num_steps 76 is less than warmup_num_steps 400
[2025-08-24 17:39:26,804] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-08-24 17:39:26,804] [WARNING] [lr_schedules.py:759:__init__] total_num_steps 76 is less than warmup_num_steps 400
[2025-08-24 17:39:26,804] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-08-24 17:39:26,805] [WARNING] [lr_schedules.py:759:__init__] total_num_steps 76 is less than warmup_num_steps 400
[2025-08-24 17:39:26,808] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-08-24 17:39:26,808] [WARNING] [lr_schedules.py:759:__init__] total_num_steps 76 is less than warmup_num_steps 400
[2025-08-24 17:39:26,808] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-08-24 17:39:26,808] [WARNING] [lr_schedules.py:759:__init__] total_num_steps 76 is less than warmup_num_steps 400
[2025-08-24 17:39:26,810] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-08-24 17:39:26,810] [WARNING] [lr_schedules.py:759:__init__] total_num_steps 76 is less than warmup_num_steps 400
[2025-08-24 17:39:26,811] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-08-24 17:39:26,811] [WARNING] [lr_schedules.py:759:__init__] total_num_steps 76 is less than warmup_num_steps 400
[2025-08-24 17:39:26,912] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-08-24 17:39:26,913] [INFO] [utils.py:782:see_memory_usage] MA 6.01 GB         Max_MA 7.97 GB         CA 10.53 GB         Max_CA 11 GB 
[2025-08-24 17:39:26,913] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 42.66 GB, percent = 1.5%
[2025-08-24 17:39:26,913] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2025-08-24 17:39:26,913] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-08-24 17:39:26,913] [WARNING] [lr_schedules.py:759:__init__] total_num_steps 76 is less than warmup_num_steps 400
[2025-08-24 17:39:26,913] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2025-08-24 17:39:26,913] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7fd8b82cbd10>
[2025-08-24 17:39:26,913] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0], mom=[[0.9, 0.999]]
[2025-08-24 17:39:26,914] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2025-08-24 17:39:26,914] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-08-24 17:39:26,914] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-08-24 17:39:26,914] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2025-08-24 17:39:26,914] [INFO] [config.py:1003:print]   amp_params ................... False
[2025-08-24 17:39:26,914] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-08-24 17:39:26,914] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2025-08-24 17:39:26,914] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2025-08-24 17:39:26,914] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2025-08-24 17:39:26,914] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2025-08-24 17:39:26,914] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2025-08-24 17:39:26,914] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd8b8150d90>
[2025-08-24 17:39:26,914] [INFO] [config.py:1003:print]   communication_data_type ...... torch.float32
[2025-08-24 17:39:26,914] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-08-24 17:39:26,914] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   dump_state ................... False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   global_rank .................. 0
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 8
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   load_universal_checkpoint .... True
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   optimizer_name ............... adamw
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   optimizer_params ............. {'lr': 2e-06, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   pld_params ................... False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   scheduler_name ............... WarmupDecayLR
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   scheduler_params ............. {'last_batch_iteration': -1, 'total_num_steps': 76, 'warmup_min_lr': 0, 'warmup_max_lr': 2e-06, 'warmup_num_steps': 400}
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   steps_per_print .............. inf
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   train_batch_size ............. 1024
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  8
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   use_node_local_storage ....... True
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   world_size ................... 16
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=1000000000 param_persistence_threshold=40960 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=8 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   zero_enabled ................. True
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2025-08-24 17:39:26,915] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[2025-08-24 17:39:26,915] [INFO] [config.py:989:print_user_config]   json = {
    "bfloat16": {
        "enabled": true
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 2e-06, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08, 
            "weight_decay": 0.0
        }
    }, 
    "scheduler": {
        "type": "WarmupDecayLR", 
        "params": {
            "last_batch_iteration": -1, 
            "total_num_steps": 76, 
            "warmup_min_lr": 0, 
            "warmup_max_lr": 2e-06, 
            "warmup_num_steps": 400
        }
    }, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "none", 
            "pin_memory": true
        }, 
        "offload_param": {
            "device": "none", 
            "pin_memory": true
        }, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_scatter": true, 
        "stage3_prefetch_bucket_size": 1.000000e+09, 
        "stage3_param_persistence_threshold": 4.096000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true, 
        "zero_hpz_partition_size": 8, 
        "zero_quantized_gradients": false, 
        "zero_quantized_weights": false, 
        "reduce_bucket_size": 1.677722e+07
    }, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 1.024000e+03, 
    "train_micro_batch_size_per_gpu": 8, 
    "communication_data_type": "fp32", 
    "wall_clock_breakdown": false, 
    "checkpoint": {
        "use_node_local_storage": true, 
        "load_universal": true
    }, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }
}
[INFO|trainer.py:2414] 2025-08-24 17:39:26,916 >> ***** Running training *****
[INFO|trainer.py:2415] 2025-08-24 17:39:26,916 >>   Num examples = 78,157
[INFO|trainer.py:2416] 2025-08-24 17:39:26,916 >>   Num Epochs = 1
[INFO|trainer.py:2417] 2025-08-24 17:39:26,916 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:2420] 2025-08-24 17:39:26,916 >>   Total train batch size (w. parallel, distributed & accumulation) = 1,024
[INFO|trainer.py:2421] 2025-08-24 17:39:26,916 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2422] 2025-08-24 17:39:26,916 >>   Total optimization steps = 76
[INFO|trainer.py:2423] 2025-08-24 17:39:26,917 >>   Number of trainable parameters = 8,030,785,536
  0%|          | 0/76 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-08-24 17:39:27,147 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[WARNING|logging.py:328] 2025-08-24 17:39:27,151 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[WARNING|logging.py:328] 2025-08-24 17:39:27,152 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[WARNING|logging.py:328] 2025-08-24 17:39:27,169 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[WARNING|logging.py:328] 2025-08-24 17:39:27,172 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[WARNING|logging.py:328] 2025-08-24 17:39:27,172 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[WARNING|logging.py:328] 2025-08-24 17:39:27,173 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[WARNING|logging.py:328] 2025-08-24 17:39:27,384 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
{'loss': 11.7452, 'grad_norm': 5.138735914624936, 'learning_rate': 0.0, 'epoch': 0.01}
{'loss': 1.4241, 'grad_norm': 4.447638919864765, 'learning_rate': 2.3137821315975918e-07, 'epoch': 0.03}
{'loss': 1.4282, 'grad_norm': 4.58015671272431, 'learning_rate': 3.6672579134208467e-07, 'epoch': 0.04}
{'loss': 1.4164, 'grad_norm': 4.9934520097148924, 'learning_rate': 4.6275642631951835e-07, 'epoch': 0.05}
{'loss': 1.3785, 'grad_norm': 4.476635651528737, 'learning_rate': 5.372435736804816e-07, 'epoch': 0.07}
{'loss': 1.3893, 'grad_norm': 4.367498686724222, 'learning_rate': 5.981040045018438e-07, 'epoch': 0.08}
{'loss': 1.3871, 'grad_norm': 4.141966558333236, 'learning_rate': 6.495607655709434e-07, 'epoch': 0.09}
{'loss': 1.4342, 'grad_norm': 3.9558280803900012, 'learning_rate': 6.941346394792774e-07, 'epoch': 0.1}
{'loss': 1.4007, 'grad_norm': 3.550935120083672, 'learning_rate': 7.334515826841693e-07, 'epoch': 0.12}
{'loss': 1.373, 'grad_norm': 3.637473701617708, 'learning_rate': 7.686217868402409e-07, 'epoch': 0.13}
{'loss': 1.3846, 'grad_norm': 3.5239541142473914, 'learning_rate': 8.004371064686714e-07, 'epoch': 0.14}
{'loss': 1.3598, 'grad_norm': 2.8619213586252115, 'learning_rate': 8.29482217661603e-07, 'epoch': 0.16}
{'loss': 1.4189, 'grad_norm': 2.7468957087310595, 'learning_rate': 8.562011298888888e-07, 'epoch': 0.17}
{'loss': 1.3681, 'grad_norm': 2.276686694440074, 'learning_rate': 8.809389787307026e-07, 'epoch': 0.18}
{'loss': 1.358, 'grad_norm': 2.245330438242981, 'learning_rate': 9.039693650225662e-07, 'epoch': 0.2}
{'loss': 1.391, 'grad_norm': 1.6537674450575137, 'learning_rate': 9.255128526390367e-07, 'epoch': 0.21}
{'loss': 1.3761, 'grad_norm': 1.8405471309209447, 'learning_rate': 9.45749848565416e-07, 'epoch': 0.22}
{'loss': 1.3173, 'grad_norm': 1.653294084116706, 'learning_rate': 9.648297958439284e-07, 'epoch': 0.24}
{'loss': 1.3089, 'grad_norm': 1.4776658814363648, 'learning_rate': 9.828778776927557e-07, 'epoch': 0.25}
{'loss': 1.32, 'grad_norm': 1.100899881800004, 'learning_rate': 1e-06, 'epoch': 0.26}
{'loss': 1.3487, 'grad_norm': 1.0179863173557917, 'learning_rate': 1.016286556913028e-06, 'epoch': 0.27}
{'loss': 1.3677, 'grad_norm': 0.9939525957535628, 'learning_rate': 1.0318153196284305e-06, 'epoch': 0.29}
{'loss': 1.3365, 'grad_norm': 0.9459688323772742, 'learning_rate': 1.0466536825099365e-06, 'epoch': 0.3}
{'loss': 1.2844, 'grad_norm': 1.0725262230708819, 'learning_rate': 1.0608604308213622e-06, 'epoch': 0.31}
{'loss': 1.3498, 'grad_norm': 1.1432302525241302, 'learning_rate': 1.0744871473609632e-06, 'epoch': 0.33}
{'loss': 1.2941, 'grad_norm': 1.0044741287919778, 'learning_rate': 1.087579343048648e-06, 'epoch': 0.34}
{'loss': 1.3391, 'grad_norm': 0.9466476124474054, 'learning_rate': 1.100177374026254e-06, 'epoch': 0.35}
{'loss': 1.2935, 'grad_norm': 0.9404587506704671, 'learning_rate': 1.1123171918904617e-06, 'epoch': 0.37}
{'loss': 1.3232, 'grad_norm': 0.9154475722148092, 'learning_rate': 1.1240309622166863e-06, 'epoch': 0.38}
{'loss': 1.3028, 'grad_norm': 0.9363005016217799, 'learning_rate': 1.1353475781823255e-06, 'epoch': 0.39}
{'loss': 1.275, 'grad_norm': 0.8000668119301566, 'learning_rate': 1.1462930899399867e-06, 'epoch': 0.41}
{'loss': 1.2975, 'grad_norm': 0.8122462948048066, 'learning_rate': 1.1568910657987959e-06, 'epoch': 0.42}
{'loss': 1.3102, 'grad_norm': 0.8048037264130707, 'learning_rate': 1.167162897810756e-06, 'epoch': 0.43}
{'loss': 1.2799, 'grad_norm': 0.7890891184373965, 'learning_rate': 1.1771280617251751e-06, 'epoch': 0.45}
{'loss': 1.3435, 'grad_norm': 0.8011772055883662, 'learning_rate': 1.1868043392514252e-06, 'epoch': 0.46}
{'loss': 1.3053, 'grad_norm': 0.7515245038856642, 'learning_rate': 1.1962080090036876e-06, 'epoch': 0.47}
{'loss': 1.2941, 'grad_norm': 0.713631013948079, 'learning_rate': 1.20535401127832e-06, 'epoch': 0.48}
{'loss': 1.2995, 'grad_norm': 0.6919341754298511, 'learning_rate': 1.2142560908525149e-06, 'epoch': 0.5}
{'loss': 1.3519, 'grad_norm': 0.7238991885075745, 'learning_rate': 1.2229269212309735e-06, 'epoch': 0.51}
{'loss': 1.2684, 'grad_norm': 0.7912283861362069, 'learning_rate': 1.2313782131597591e-06, 'epoch': 0.52}
{'loss': 1.3386, 'grad_norm': 0.7049335111932267, 'learning_rate': 1.2396208097390182e-06, 'epoch': 0.54}
{'loss': 1.3053, 'grad_norm': 0.6735369189164008, 'learning_rate': 1.2476647700727873e-06, 'epoch': 0.55}
{'loss': 1.289, 'grad_norm': 0.7074327775380753, 'learning_rate': 1.2555194430747504e-06, 'epoch': 0.56}
{'loss': 1.3034, 'grad_norm': 0.6931254649109898, 'learning_rate': 1.2631935327881894e-06, 'epoch': 0.58}
{'loss': 1.2954, 'grad_norm': 0.7105083193137831, 'learning_rate': 1.2706951563646508e-06, 'epoch': 0.59}
{'loss': 1.2995, 'grad_norm': 0.6615191489402101, 'learning_rate': 1.2780318956696959e-06, 'epoch': 0.6}
{'loss': 1.2544, 'grad_norm': 0.7007841597494595, 'learning_rate': 1.2852108433382902e-06, 'epoch': 0.62}
{'loss': 1.2578, 'grad_norm': 0.6869214306880693, 'learning_rate': 1.2922386439811214e-06, 'epoch': 0.63}
{'loss': 1.2703, 'grad_norm': 0.6304954478065191, 'learning_rate': 1.2991215311418868e-06, 'epoch': 0.64}
{'loss': 1.3223, 'grad_norm': 0.6454381078498339, 'learning_rate': 1.3058653605207224e-06, 'epoch': 0.65}
{'loss': 1.2613, 'grad_norm': 0.6714788754015418, 'learning_rate': 1.3124756399075005e-06, 'epoch': 0.67}
{'loss': 1.2545, 'grad_norm': 0.7009706324805919, 'learning_rate': 1.3189575562084071e-06, 'epoch': 0.68}
{'loss': 1.2818, 'grad_norm': 0.6584017320127719, 'learning_rate': 1.3253159998980685e-06, 'epoch': 0.69}
{'loss': 1.2824, 'grad_norm': 0.669362443244134, 'learning_rate': 1.3315555871860131e-06, 'epoch': 0.71}
{'loss': 1.2603, 'grad_norm': 0.6657093212705266, 'learning_rate': 1.337680680149153e-06, 'epoch': 0.72}
{'loss': 1.2783, 'grad_norm': 0.6530936070314408, 'learning_rate': 1.3436954050502211e-06, 'epoch': 0.73}
{'loss': 1.2415, 'grad_norm': 0.6549733721191674, 'learning_rate': 1.3496036690348402e-06, 'epoch': 0.75}
{'loss': 1.2713, 'grad_norm': 0.6475423088347275, 'learning_rate': 1.3554091753764455e-06, 'epoch': 0.76}
{'loss': 1.2273, 'grad_norm': 0.6760292516540543, 'learning_rate': 1.3611154374180198e-06, 'epoch': 0.77}
{'loss': 1.2287, 'grad_norm': 0.6300503830188111, 'learning_rate': 1.3667257913420845e-06, 'epoch': 0.79}
{'loss': 1.2634, 'grad_norm': 0.6595551538415304, 'learning_rate': 1.372243407885168e-06, 'epoch': 0.8}
{'loss': 1.2468, 'grad_norm': 0.6541841288944794, 'learning_rate': 1.377671303099746e-06, 'epoch': 0.81}
{'loss': 1.2759, 'grad_norm': 0.652685675914951, 'learning_rate': 1.3830123482551126e-06, 'epoch': 0.82}
{'loss': 1.2423, 'grad_norm': 0.6486034214376624, 'learning_rate': 1.3882692789585548e-06, 'epoch': 0.84}
{'loss': 1.2826, 'grad_norm': 0.6524722809097808, 'learning_rate': 1.3934447035693704e-06, 'epoch': 0.85}
{'loss': 1.3195, 'grad_norm': 0.6657596393154577, 'learning_rate': 1.398541110970515e-06, 'epoch': 0.86}
{'loss': 1.3254, 'grad_norm': 0.6180628310035882, 'learning_rate': 1.4035608777558495e-06, 'epoch': 0.88}
{'loss': 1.2908, 'grad_norm': 0.6694216066829791, 'learning_rate': 1.4085062748849343e-06, 'epoch': 0.89}
{'loss': 1.2067, 'grad_norm': 0.6488598352723675, 'learning_rate': 1.4133794738520214e-06, 'epoch': 0.9}
{'loss': 1.2378, 'grad_norm': 0.6161767786629705, 'learning_rate': 1.4181825524111844e-06, 'epoch': 0.92}
{'loss': 1.2982, 'grad_norm': 0.6565547812995524, 'learning_rate': 1.4229174998953693e-06, 'epoch': 0.93}
{'loss': 1.3209, 'grad_norm': 0.6732554374543254, 'learning_rate': 1.4275862221634468e-06, 'epoch': 0.94}
{'loss': 1.2869, 'grad_norm': 0.653937783631565, 'learning_rate': 1.4321905462060529e-06, 'epoch': 0.96}
{'loss': 1.3024, 'grad_norm': 0.7137463556300175, 'learning_rate': 1.4367322244380792e-06, 'epoch': 0.97}
{'loss': 1.2536, 'grad_norm': 0.6072994610749171, 'learning_rate': 1.441212938703048e-06, 'epoch': 0.98}
{'loss': 1.2717, 'grad_norm': 0.6602216022781978, 'learning_rate': 1.445634304012274e-06, 'epoch': 1.0}
  1%|▏         | 1/76 [00:52<1:05:57, 52.77s/it]                                                  1%|▏         | 1/76 [00:52<1:05:57, 52.77s/it]  3%|▎         | 2/76 [01:47<1:06:16, 53.74s/it]                                                  3%|▎         | 2/76 [01:47<1:06:16, 53.74s/it]  4%|▍         | 3/76 [02:42<1:06:00, 54.26s/it]                                                  4%|▍         | 3/76 [02:42<1:06:00, 54.26s/it]  5%|▌         | 4/76 [03:36<1:05:07, 54.27s/it]                                                  5%|▌         | 4/76 [03:36<1:05:07, 54.27s/it]  7%|▋         | 5/76 [04:30<1:04:15, 54.30s/it]                                                  7%|▋         | 5/76 [04:30<1:04:15, 54.30s/it]  8%|▊         | 6/76 [05:24<1:03:19, 54.28s/it]                                                  8%|▊         | 6/76 [05:24<1:03:19, 54.28s/it]  9%|▉         | 7/76 [06:19<1:02:39, 54.48s/it]                                                  9%|▉         | 7/76 [06:19<1:02:39, 54.48s/it] 11%|█         | 8/76 [07:14<1:01:41, 54.44s/it]                                                 11%|█         | 8/76 [07:14<1:01:41, 54.44s/it] 12%|█▏        | 9/76 [08:08<1:00:44, 54.40s/it]                                                 12%|█▏        | 9/76 [08:08<1:00:44, 54.40s/it] 13%|█▎        | 10/76 [09:03<59:55, 54.47s/it]                                                 13%|█▎        | 10/76 [09:03<59:55, 54.47s/it] 14%|█▍        | 11/76 [09:57<59:01, 54.48s/it]                                                14%|█▍        | 11/76 [09:57<59:01, 54.48s/it] 16%|█▌        | 12/76 [10:51<58:04, 54.44s/it]                                                16%|█▌        | 12/76 [10:51<58:04, 54.44s/it] 17%|█▋        | 13/76 [11:46<57:07, 54.40s/it]                                                17%|█▋        | 13/76 [11:46<57:07, 54.40s/it] 18%|█▊        | 14/76 [12:41<56:21, 54.53s/it]                                                18%|█▊        | 14/76 [12:41<56:21, 54.53s/it] 20%|█▉        | 15/76 [13:35<55:21, 54.46s/it]                                                20%|█▉        | 15/76 [13:35<55:21, 54.46s/it] 21%|██        | 16/76 [14:29<54:24, 54.41s/it]                                                21%|██        | 16/76 [14:29<54:24, 54.41s/it] 22%|██▏       | 17/76 [15:24<53:27, 54.37s/it]                                                22%|██▏       | 17/76 [15:24<53:27, 54.37s/it] 24%|██▎       | 18/76 [16:18<52:40, 54.50s/it]                                                24%|██▎       | 18/76 [16:18<52:40, 54.50s/it] 25%|██▌       | 19/76 [17:13<51:44, 54.47s/it]                                                25%|██▌       | 19/76 [17:13<51:44, 54.47s/it] 26%|██▋       | 20/76 [18:07<50:49, 54.45s/it]                                                26%|██▋       | 20/76 [18:07<50:49, 54.45s/it] 28%|██▊       | 21/76 [19:02<49:57, 54.51s/it]                                                28%|██▊       | 21/76 [19:02<49:57, 54.51s/it] 29%|██▉       | 22/76 [19:56<49:02, 54.50s/it]                                                29%|██▉       | 22/76 [19:56<49:02, 54.50s/it] 30%|███       | 23/76 [20:50<48:04, 54.42s/it]                                                30%|███       | 23/76 [20:50<48:04, 54.42s/it] 32%|███▏      | 24/76 [21:45<47:09, 54.40s/it]                                                32%|███▏      | 24/76 [21:45<47:09, 54.40s/it] 33%|███▎      | 25/76 [22:40<46:20, 54.52s/it]                                                33%|███▎      | 25/76 [22:40<46:20, 54.52s/it] 34%|███▍      | 26/76 [23:34<45:23, 54.47s/it]                                                34%|███▍      | 26/76 [23:34<45:23, 54.47s/it] 36%|███▌      | 27/76 [24:28<44:27, 54.43s/it]                                                36%|███▌      | 27/76 [24:28<44:27, 54.43s/it] 37%|███▋      | 28/76 [25:23<43:32, 54.43s/it]                                                37%|███▋      | 28/76 [25:23<43:32, 54.43s/it] 38%|███▊      | 29/76 [26:17<42:43, 54.53s/it]                                                38%|███▊      | 29/76 [26:17<42:43, 54.53s/it] 39%|███▉      | 30/76 [27:12<41:45, 54.47s/it]                                                39%|███▉      | 30/76 [27:12<41:45, 54.47s/it] 41%|████      | 31/76 [28:06<40:48, 54.41s/it]                                                41%|████      | 31/76 [28:06<40:48, 54.41s/it] 42%|████▏     | 32/76 [29:01<39:57, 54.48s/it]                                                42%|████▏     | 32/76 [29:01<39:57, 54.48s/it] 43%|████▎     | 33/76 [29:55<39:03, 54.49s/it]                                                43%|████▎     | 33/76 [29:55<39:03, 54.49s/it] 45%|████▍     | 34/76 [30:50<38:06, 54.43s/it]                                                45%|████▍     | 34/76 [30:50<38:06, 54.43s/it] 46%|████▌     | 35/76 [31:44<37:10, 54.40s/it]                                                46%|████▌     | 35/76 [31:44<37:10, 54.40s/it] 47%|████▋     | 36/76 [32:39<36:20, 54.52s/it]                                                47%|████▋     | 36/76 [32:39<36:20, 54.52s/it] 49%|████▊     | 37/76 [33:33<35:24, 54.48s/it]                                                49%|████▊     | 37/76 [33:33<35:24, 54.48s/it] 50%|█████     | 38/76 [34:27<34:29, 54.47s/it]                                                50%|█████     | 38/76 [34:27<34:29, 54.47s/it] 51%|█████▏    | 39/76 [35:22<33:34, 54.44s/it]                                                51%|█████▏    | 39/76 [35:22<33:34, 54.44s/it] 53%|█████▎    | 40/76 [36:17<32:42, 54.51s/it]                                                53%|█████▎    | 40/76 [36:17<32:42, 54.51s/it] 54%|█████▍    | 41/76 [37:11<31:46, 54.46s/it]                                                54%|█████▍    | 41/76 [37:11<31:46, 54.46s/it] 55%|█████▌    | 42/76 [38:05<30:50, 54.42s/it]                                                55%|█████▌    | 42/76 [38:05<30:50, 54.42s/it] 57%|█████▋    | 43/76 [39:00<29:57, 54.48s/it]                                                57%|█████▋    | 43/76 [39:00<29:57, 54.48s/it] 58%|█████▊    | 44/76 [39:54<29:04, 54.50s/it]                                                58%|█████▊    | 44/76 [39:54<29:04, 54.50s/it] 59%|█████▉    | 45/76 [40:49<28:08, 54.46s/it]                                                59%|█████▉    | 45/76 [40:49<28:08, 54.46s/it] 61%|██████    | 46/76 [41:43<27:12, 54.43s/it]                                                61%|██████    | 46/76 [41:43<27:12, 54.43s/it] 62%|██████▏   | 47/76 [42:38<26:22, 54.57s/it]                                                62%|██████▏   | 47/76 [42:38<26:22, 54.57s/it] 63%|██████▎   | 48/76 [43:32<25:25, 54.48s/it]                                                63%|██████▎   | 48/76 [43:32<25:25, 54.48s/it] 64%|██████▍   | 49/76 [44:27<24:29, 54.43s/it]                                                64%|██████▍   | 49/76 [44:27<24:29, 54.43s/it] 66%|██████▌   | 50/76 [45:21<23:34, 54.40s/it]                                                66%|██████▌   | 50/76 [45:21<23:34, 54.40s/it] 67%|██████▋   | 51/76 [46:16<22:43, 54.53s/it]                                                67%|██████▋   | 51/76 [46:16<22:43, 54.53s/it] 68%|██████▊   | 52/76 [47:10<21:46, 54.46s/it]                                                68%|██████▊   | 52/76 [47:10<21:46, 54.46s/it] 70%|██████▉   | 53/76 [48:04<20:51, 54.40s/it]                                                70%|██████▉   | 53/76 [48:04<20:51, 54.40s/it] 71%|███████   | 54/76 [48:59<19:58, 54.47s/it]                                                71%|███████   | 54/76 [48:59<19:58, 54.47s/it] 72%|███████▏  | 55/76 [49:53<19:04, 54.48s/it]                                                72%|███████▏  | 55/76 [49:53<19:04, 54.48s/it] 74%|███████▎  | 56/76 [50:48<18:08, 54.44s/it]                                                74%|███████▎  | 56/76 [50:48<18:08, 54.44s/it] 75%|███████▌  | 57/76 [51:42<17:13, 54.39s/it]                                                75%|███████▌  | 57/76 [51:42<17:13, 54.39s/it] 76%|███████▋  | 58/76 [52:37<16:21, 54.53s/it]                                                76%|███████▋  | 58/76 [52:37<16:21, 54.53s/it] 78%|███████▊  | 59/76 [53:31<15:25, 54.45s/it]                                                78%|███████▊  | 59/76 [53:31<15:25, 54.45s/it] 79%|███████▉  | 60/76 [54:26<14:30, 54.42s/it]                                                79%|███████▉  | 60/76 [54:26<14:30, 54.42s/it] 80%|████████  | 61/76 [55:20<13:35, 54.40s/it]                                                80%|████████  | 61/76 [55:20<13:35, 54.40s/it] 82%|████████▏ | 62/76 [56:15<12:42, 54.49s/it]                                                82%|████████▏ | 62/76 [56:15<12:42, 54.49s/it] 83%|████████▎ | 63/76 [57:09<11:47, 54.43s/it]                                                83%|████████▎ | 63/76 [57:09<11:47, 54.43s/it] 84%|████████▍ | 64/76 [58:03<10:53, 54.43s/it]                                                84%|████████▍ | 64/76 [58:03<10:53, 54.43s/it] 86%|████████▌ | 65/76 [58:58<09:59, 54.52s/it]                                                86%|████████▌ | 65/76 [58:58<09:59, 54.52s/it] 87%|████████▋ | 66/76 [59:53<09:05, 54.51s/it]                                                87%|████████▋ | 66/76 [59:53<09:05, 54.51s/it] 88%|████████▊ | 67/76 [1:00:47<08:09, 54.43s/it]                                                  88%|████████▊ | 67/76 [1:00:47<08:09, 54.43s/it] 89%|████████▉ | 68/76 [1:01:41<07:14, 54.37s/it]                                                  89%|████████▉ | 68/76 [1:01:41<07:14, 54.37s/it] 91%|█████████ | 69/76 [1:02:36<06:21, 54.51s/it]                                                  91%|█████████ | 69/76 [1:02:36<06:21, 54.51s/it] 92%|█████████▏| 70/76 [1:03:30<05:26, 54.44s/it]                                                  92%|█████████▏| 70/76 [1:03:30<05:26, 54.44s/it] 93%|█████████▎| 71/76 [1:04:24<04:32, 54.41s/it]                                                  93%|█████████▎| 71/76 [1:04:24<04:32, 54.41s/it] 95%|█████████▍| 72/76 [1:05:19<03:37, 54.41s/it]                                                  95%|█████████▍| 72/76 [1:05:19<03:37, 54.41s/it] 96%|█████████▌| 73/76 [1:06:14<02:43, 54.54s/it]                                                  96%|█████████▌| 73/76 [1:06:14<02:43, 54.54s/it] 97%|█████████▋| 74/76 [1:07:08<01:48, 54.48s/it]                                                  97%|█████████▋| 74/76 [1:07:08<01:48, 54.48s/it] 99%|█████████▊| 75/76 [1:08:02<00:54, 54.45s/it]                                                  99%|█████████▊| 75/76 [1:08:02<00:54, 54.45s/it]100%|██████████| 76/76 [1:08:57<00:00, 54.52s/it]                                                 100%|██████████| 76/76 [1:08:57<00:00, 54.52s/it][INFO|trainer.py:3984] 2025-08-24 18:48:33,107 >> Saving model checkpoint to /mount/local-checkpoints/checkpoint-76
[INFO|configuration_utils.py:419] 2025-08-24 18:48:33,109 >> Configuration saved in /mount/local-checkpoints/checkpoint-76/config.json
[INFO|configuration_utils.py:911] 2025-08-24 18:48:33,110 >> Configuration saved in /mount/local-checkpoints/checkpoint-76/generation_config.json
[INFO|modeling_utils.py:3580] 2025-08-24 18:48:47,704 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /mount/local-checkpoints/checkpoint-76/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2510] 2025-08-24 18:48:47,706 >> tokenizer config file saved in /mount/local-checkpoints/checkpoint-76/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-08-24 18:48:47,706 >> Special tokens file saved in /mount/local-checkpoints/checkpoint-76/special_tokens_map.json
[2025-08-24 18:48:47,832] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step76 is about to be saved!
[2025-08-24 18:48:47,839] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /mount/local-checkpoints/checkpoint-76/global_step76/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-08-24 18:48:47,839] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /mount/local-checkpoints/checkpoint-76/global_step76/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-08-24 18:48:48,134] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /mount/local-checkpoints/checkpoint-76/global_step76/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-08-24 18:48:48,142] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /mount/local-checkpoints/checkpoint-76/global_step76/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-08-24 18:48:55,698] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /mount/local-checkpoints/checkpoint-76/global_step76/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-08-24 18:48:55,698] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /mount/local-checkpoints/checkpoint-76/global_step76/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-08-24 18:48:55,841] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step76 is ready now!
[INFO|trainer.py:2681] 2025-08-24 18:48:55,844 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 4168.9274, 'train_samples_per_second': 18.748, 'train_steps_per_second': 0.018, 'train_loss': 1.449902970539896, 'epoch': 1.0}
                                                 100%|██████████| 76/76 [1:09:28<00:00, 54.52s/it]100%|██████████| 76/76 [1:09:29<00:00, 54.87s/it]
[INFO|trainer.py:3984] 2025-08-24 18:49:05,209 >> Saving model checkpoint to /mount/local-checkpoints
[INFO|configuration_utils.py:419] 2025-08-24 18:49:05,211 >> Configuration saved in /mount/local-checkpoints/config.json
[INFO|configuration_utils.py:911] 2025-08-24 18:49:05,212 >> Configuration saved in /mount/local-checkpoints/generation_config.json
[2025-08-24 18:49:07,527] [INFO] [launch.py:351:main] Process 1758 exits successfully.
[2025-08-24 18:49:07,528] [INFO] [launch.py:351:main] Process 1763 exits successfully.
[2025-08-24 18:49:07,528] [INFO] [launch.py:351:main] Process 1759 exits successfully.
[2025-08-24 18:49:08,528] [INFO] [launch.py:351:main] Process 1760 exits successfully.
[2025-08-24 18:49:08,529] [INFO] [launch.py:351:main] Process 1762 exits successfully.
[2025-08-24 18:49:08,529] [INFO] [launch.py:351:main] Process 1757 exits successfully.
[2025-08-24 18:49:08,529] [INFO] [launch.py:351:main] Process 1761 exits successfully.
[INFO|modeling_utils.py:3580] 2025-08-24 18:49:20,321 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /mount/local-checkpoints/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2510] 2025-08-24 18:49:20,323 >> tokenizer config file saved in /mount/local-checkpoints/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-08-24 18:49:20,324 >> Special tokens file saved in /mount/local-checkpoints/special_tokens_map.json
***** train metrics *****
  epoch                    =     0.9951
  total_flos               =   693776GF
  train_loss               =     1.4499
  train_runtime            = 1:09:28.92
  train_samples            =      78157
  train_samples_per_second =     18.748
  train_steps_per_second   =      0.018
[INFO|modelcard.py:450] 2025-08-24 18:49:20,778 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[2025-08-24 18:49:22,531] [INFO] [launch.py:351:main] Process 1756 exits successfully.
Training completed.
Training finished. Copying checkpoints to persistent storage.
/bin/bash: line 12: [: ${NODE_RANK}: integer expression expected
Not rank 0, skipping copy.
